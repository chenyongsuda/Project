1.导入平面文件
 # Load file as array from numpy: digits
      digits = np.loadtxt(file, delimiter=',')
      data = np.loadtxt(file, delimiter=',', skiprows=1, usecols=[0,2])
      data_float = np.loadtxt(file, delimiter='\t', dtype=float, skiprows=1)
      从文本推测类型使用dtype=None
      data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)
      data is an object called a structured array
      注意numpy都是类型array([])
      Accessing rows and columns of structured arrays is super-intuitive: to get the ith row, 
      merely execute data[i] and to get the column with name 'Fare', execute data['Fare']
      
      d = np.recfromcsv('titanic.csv')
      using the function np.recfromcsv() and assign it to the variable, d. 
      You'll only need to pass file to it because it has the defaults delimiter=',' and names=True in addition to dtype=None!
 
 #Load File From Pandas(Best Pratice in Data Anaslsy)
      #Load Excel
          df = pd.read_csv(file)
          df.values =======>convert to numpy

          # Read the first 5 rows of the file into a DataFrame: data
          data = pd.read_csv(file,nrows=5,header=None)

          takes characters that comments occur after in the file, which in this case is '#'. na_values takes a list of strings to recognize as NA/NaN, in this case the string 'Nothing'.
          data = pd.read_csv(file, sep='\t', comment='#', na_values=['Nothing'])
      
      #Load Excel
          # Load spreadsheet: xl
          xl = pd.ExcelFile(file)
          # Print sheet names
          print(xl.sheet_names)
          
          # Load a sheet into a DataFrame by name: df1
          df1 = xl.parse('2004')
          
          # Parse the first column of the second sheet and rename the column: df2
          # 只导出一个列,并且给这个列取名Country
          df2 = xl.parse(1, parse_cols=[0], skiprows=[0], names=['Country'])
          
      #Importing SAS/Stata files using pandas
          # Import sas7bdat package
          from sas7bdat import SAS7BDAT

          # Save file to a DataFrame: df_sas
          with SAS7BDAT('sales.sas7bdat') as file:
              df_sas = file.to_data_frame()
          
          #读取分析文件
          df = pd.read_stata('disarea.dta')
          
      #读取HDF5
          import numpy as np
          import h5py
          # Assign filename: file
          # Load file: data
          data = h5py.File('LIGO_data.hdf5', 'r')
          # Print the datatype of the loaded file
          print(type(data))
          # Print the keys of the file
          for key in data.keys():
              print(key)


       #Importing MATLAB files
          # Import package
          import scipy.io
          # Load MATLAB file: mat
          mat = scipy.io.loadmat('albeck_gene_expression.mat')
          # Print the datatype type of mat
          print(type(mat))
  
       #连接数据库文件
          # Import necessary module
          from sqlalchemy import create_engine
          # Create engine: engine
          engine = create_engine('sqlite:///Chinook.sqlite')
          Save the table names to a list: table_names
          table_names = engine.table_names()
          # Print the table names to the shell
          print(table_names)
          
       #查询数据
          # Open engine in context manager
          engine = create_engine('sqlite:///Chinook.sqlite')
          # Perform query and save results to DataFrame: df
          with engine.connect() as con:
              rs = con.execute("SELECT LastName, Title FROM Employee")
              df = pd.DataFrame(rs.fetchmany(size=3))
              df.columns = rs.keys()
          # Print the length of the DataFrame df
          print(len(df))
          # Print the head of the DataFrame df
          print(df.head())
        
       #简单的读取数据库数据到pandas
          # Create engine: engine
          engine = create_engine('sqlite:///Chinook.sqlite')
          # Execute query and store records in DataFrame: df
          df = pd.read_sql_query('SELECT * FROM Album', engine)
          
       #Load Data From web
          ---URLRETRIVE
          # Import package
          from urllib.request import urlretrieve
          # Import pandas
          import pandas as pd
          # Assign url of file: url
          url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
          # Save file locally
          urlretrieve(url,'winequality-red.csv')
          # Read file into a DataFrame and print its head
          df = pd.read_csv('winequality-red.csv', sep=';')
          print(df.head())
          # Print head of DataFrame
          print(df.head())
        
          ---URLOpen方法
          import urllib
          url = "http://www.baidu.com/"
          #urlopen()
          sock = urllib.urlopen(url)
          htmlCode = sock.read()
          sock.close
          fp = open("e:/1.html","wb")
          fp.write(htmlCode)
          fp.close
          --------urlretrieve()
          urllib.urlretrieve(url, 'e:/2.html')
          read() , readline() , readlines() , fileno() , close() ：这些方法的使用方式与文件对象完全一样;
          info()：返回一个httplib.HTTPMessage 对象，表示远程服务器返回的头信息
          getcode()：返回Http状态码。如果是http请求，200表示请求成功完成;404表示网址未找到；
          geturl()：返回请求的url；
          
          ---------直接读取远程文件
          # Assign url of file: url
          url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'
          # Read file into a DataFrame: df
          df = pd.read_csv(url,sep=';')
          
          ---------远程读取EXCEL
          # Import package
          import pandas as pd
          # Assign url of file: url
          url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'
          # Read in all sheets of Excel file: xl
          xl = pd.read_excel(url,sheetname=None)
          # Print the sheetnames to the shell
          print(xl.keys())
          # Print the head of the first sheet (using its name, NOT its index)
          print(xl['1700'].head())
          
          ----------说明
          Note that the output of pd.read_excel() is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.
          in order to import all sheets you need to pass None to the argument sheetname.
          
          -----------用URLLib读取数据
          # Import packages
          from urllib.request import urlopen, Request
          # Specify the url
          url = "http://www.datacamp.com/teach/documentation"
          # This packages the request
          request = Request(url)
          # Sends the request and catches the response: response
          response = urlopen(request)
          # Extract the response: html
          html = response.read()
          # Print the html
          print(html)
          # Be polite and close the response!
          response.close()
          
          -------------用requests包
          # Import package
          import requests
          # Specify the url: url
          url = "http://www.datacamp.com/teach/documentation"
          # Packages the request, send the request and catch the response: r
          r = requests.get(url)
          # Extract the response: text
          text = r.text
          # Print the html
          print(text)
          
          ---------------用beautifulsoap解析html
          # Import packages
          import requests
          from bs4 import BeautifulSoup
          # Specify url: url
          url = 'https://www.python.org/~guido/'
          # Package the request, send the request and catch the response: r
          r = requests.get(url)
          # Extracts the response as html: html_doc
          html_doc = r.text
          # Create a BeautifulSoup object from the HTML: soup
          soup = BeautifulSoup(html_doc)
          # Prettify the BeautifulSoup object: pretty_soup
          pretty_soup = soup.prettify()
          # Print the response
          print(pretty_soup)

          # Import packages
          import requests
          from bs4 import BeautifulSoup
          # Specify url: url
          url = 'https://www.python.org/~guido/'
          # Package the request, send the request and catch the response: r
          r = requests.get(url)
          # Extract the response as html: html_doc
          html_doc = r.text
          # Create a BeautifulSoup object from the HTML: soup
          soup = BeautifulSoup(html_doc)
          # Get the title of Guido's webpage: guido_title
          guido_title = soup.title
          # Print the title of Guido's webpage to the shell
          print(guido_title)
          # Get Guido's text: guido_text
          guido_text = soup.get_text()
          # Print Guido's text to the shell
          print(guido_text)
          print(soup.prettify())

          获取属性
          link.get('href')
          
 #Load Json from file
      json_data = json.load(json_file)
      
      www.omdbapi.com 开源电源数据api
      构造Dataframe from dict
      Now you have the Twitter data in a list of dictionaries, tweets_data, 
      where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet.
      The text in a tweet, t1, is stored as the value t1['text']; similarly, the language is stored in t1['lang']. 
      Your task is to build a DataFrame in which each row is a tweet and the columns are 'text' and 'lang'.
      
      # Build DataFrame of tweet texts and languages
      df = pd.DataFrame(tweets_data, columns=['text','lang'])
      
      迭代dataframe
      for index, row in df.iterrows():
