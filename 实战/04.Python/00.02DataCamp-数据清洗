1. 数据存在哪些质量问题
  # 需要调整的columns,包括column的字段多空格啊等
  # 数据丢失 有一些行是none啊等
  # 数据值无效或者异常值
  # 重复行
  # 需要处理后才能分析的列

2. 一般通过
df.columns 看出columns的一些情况
df.shape() 看出一共多少数据
df.info() 看一看出各列非none多少行等信息
df.xxx.value_counts(dropna=False) 判断某列值的评率
df.discribe() 查看数值型类的均值最大最小等信息

3.统计数据可视化
比如直方图
      df.xxxxxx.plot('hist')
      import matplotlib.pyplot as plt
      plt.show()
      这样直接看图中的异常值.
      通过df[df.xxxxxx > xxxxx]来浏览数据
      
比如箱线图可以看出异常点和25,50,75,100范围值
      Histograms are great ways of visualizing single variables. To visualize multiple variables, boxplots are useful, 
      especially when one of the variables is categorical.
      # Create the boxplot
      df.boxplot(column='initial_cost', by='Borough', rot=90)
      
 点图比较适合两个比较
      Boxplots are great when you have a numeric column that you want to compare across different categories. When you want to visualize two numeric columns, scatter plots are ideal.
      # Import necessary modules
      import pandas as pd
      import matplotlib.pyplot as plt
      # Create and display the first scatter plot
      df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)
      plt.show()
      # Create and display the second scatter plot
      df_subset.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)
      plt.show()
      
4. 整洁的数据包含哪几方面
    每个变量作为单独的列。
    Each variable as a separate column.
    每行作为一个单独的观察。
    Each row as a separate observation.
    使用pd.melt方法类似于转置操作,将列的值转置为行数据,含义是将多列溶解为类型列一个值域
    
    举例:
    In this exercise, you will practice melting a DataFrame using pd.melt(). There are two parameters you should be aware of: 
    id_vars and value_vars. The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape), 
    while the value_vars represent the columns you do wish to melt into rows. By default, if no value_vars are provided, 
    all columns not set in the id_vars will be melted. This could save a bit of typing, 
    depending on the number of columns that need to be melted.
    # Melt airquality: airquality_melt
    airquality_melt = pd.melt(airquality, id_vars=['Month','Day'])
    
    # Melt airquality: airquality_melt
    Melt the columns of airquality with the defaultvariablecolumn renamed to'measurement'and the defaultvaluecolumn renamed to'reading'. 
    You can do this by specifying, respectively, thevarnameandvaluename` parameters.
    airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')
    
    melt的反向操作pivot
    # Pivot airquality_melt: airquality_pivot
    # pivot_table() has an index parameter which you can use to specify the columns that you don't want pivoted: 
    # It is similar to the id_vars parameter of pd.melt(). Two other parameters that you have to specify are columns
    # (the name of the column you want to pivot), and values (the values to be used when the column is pivoted). 
    # The melted DataFrame airquality_melt has been pre-loaded for you.
    airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')
    
    pivot后Index会变成复合Index
    可以参考下层次化索引
    参考 https://blog.csdn.net/sinat_29957455/article/details/79028730
    层次化索引(hierarchical indexing)是pandas的一个重要的功能，它可以在一个轴上有多个（两个以上）的索引，这就表示着，它能够以低维度形式来表示高维度的数据。
    #There's a very simple method you can use to get back the original DataFrame from the pivoted DataFrame:
    .reset_index(). Dan didn't show you how to use this method in the video, 
    but you're now going to practice using it in this exercise to get back the original DataFrame from airquality_pivot,
    which has been pre-loaded.
    airquality_pivot_reset = airquality_pivot.reset_index()
    
    pivot后如果数据有多条的话需要使用聚合函数
    # Pivot table the airquality_dup: airquality_pivot 
    airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)
    
    切分字段：
    In order to parse this value, you need to extract the first letter into a new column for gender, 
    and the rest into a column for age_group. Here, since you can parse values by position, 
    you can take advantage of pandas' vectorized string slicing by using the str attribute of columns of type object.
    # Melt tb: tb_melt
    tb_melt = pd.melt(tb, id_vars=['country', 'year'])
    # Create the 'gender' column
    tb_melt['gender'] = tb_melt.variable.str[0]
    # Create the 'age_group' column
    tb_melt['age_group'] = tb_melt.variable.str[1:]
    # Print the head of tb_melt
    print(tb_melt.head())
  
    进入某个字段使用str进入后如果是string的话可以使用string特有的操作 如果是list的话可以使用list的一些操作比如get(0) get(1)等
    例子：
    # Melt ebola: ebola_melt
    ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')
    print(ebola_melt)
    # Create the 'str_split' column
    ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')
    # Create the 'type' column
    ebola_melt['type'] = ebola_melt.str_split.str[0]
    # Create the 'country' column
    ebola_melt['country'] = ebola_melt.str_split.str[1]
    # Print the head of ebola_melt
    print(ebola_melt.head())
    
    合并数据源,可以行级别合并和列级别合并
    # Row级别合并
    # Concatenate uber1, uber2, and uber3: row_concat
    row_concat = pd.concat([uber1,uber2,uber3])
    
    # Column级别合并
    # Concatenate ebola_melt and status_country column-wise: ebola_tidy
    ebola_tidy = pd.concat([ebola_melt,status_country],axis=1)
    
    # 忽略index合并,不忽略的话index重复,忽略的话自动生成新的index
    # Concatenate uber1, uber2, and uber3: row_concat
    row_concat = pd.concat([uber1,uber2,uber3],ignore_index=true)
    
    #获取glob匹配的文件然后构造dataframe
    # Import necessary modules
    import glob
    import pandas as pd
    # Write the pattern: pattern
    pattern = '*.csv'
    # Save all file matches: csv_files
    csv_files = glob.glob(pattern)
    # Print the file names
    print(csv_files)
    # Load the second file into a DataFrame: csv2
    csv2 = pd.read_csv(csv_files[1])
    # Print the head of csv2
    print(csv2.head())

    #合并每一个dataframe
    # Create an empty list: frames
    frames = []
    #  Iterate over csv_files
    for csv in csv_files:
    #  Read csv into a DataFrame: df
    df = pd.read_csv(csv)
    # Append df to frames
    frames.append(df)
    # Concatenate frames into a single DataFrame: uber
    uber = pd.concat(frames)
    # Print the shape of uber
    print(uber.shape)
    # Print the head of uber
    print(uber.head())
    
    #合并列数据的另一种方式
    #之前方式列合并两个dataframe是靠index 现在如果有相同的column的话但是这个column的顺序不一致,如何合并呢
    pandas有merge on 方法,类似于SQL中的join on...
    # Merge the DataFrames: o2o
    o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')
    
    --------数据类型转换
    # Convert the sex column to type 'category'
    tips.sex = tips.sex.astype('category')
    # Convert the smoker column to type 'category'
    tips.smoker = tips.smoker.astype('category')
    
    #转成对象类型
    tips.smoker.astype('str')
    
    #转换成数字类型或者浮点类型
    If you expect the data type of a column to be numeric (int or float), but instead it is of type object, this typically means that there is a non numeric value in the column, which also signifies bad data.
    You can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. 
    You can either use the techniques you learned in Chapter 1 to do some exploratory data analysis and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.
    # Convert 'total_bill' to a numeric dtype
    tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')
    # Convert 'tip' to a numeric dtype
    tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')
    
    
    ---------正则表达式
    # Compile the pattern: prog
    prog = re.compile('\d{3}-\d{3}-\d{4}')
    # See if the pattern matches
    result = prog.match('123
    
    # 提取出所有的满足条件的数字
    When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), 
    you can use the re.findall() function. Dan did not discuss this in the video, but it is straightforward to use: 
    You pass in a pattern and a string to re.findall(), and it will return a list of the matches.
    # Import the regular expression module
    import re
    # Find the numeric values: matches
    matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')

    #数据操作Apply 可以serise操作或者Dataframe操作
    # serise操作
    # Define recode_gender()
    def recode_gender(gender):
        # Return 0 if gender is 'Female'
        if gender == 'Female':
            return 0
        # Return 1 if gender is 'Male'    
        elif gender == 'Male':
            return 1
        # Return np.nan    
        else:
            return np.nan
    # Apply the function to the sex column
    tips['recode'] = tips.sex.apply(recode_gender)
    # Print the first five rows of tips
    print(tips.head())
    
    #使用lambda表达式
    # Write the lambda function using replace
    tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))
    # Write the lambda function using regular expressions
    tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])
    
    #删除重复的值/填新的值
    Dropping duplicate data
    # Create the new DataFrame: tracks
    tracks = billboard[['year', 'artist', 'track', 'time']]
    # Drop the duplicates: tracks_no_duplicates
    tracks_no_duplicates = tracks.drop_duplicates()
    
    #填入新值
    # Calculate the mean of the Ozone column: oz_mean
    oz_mean = airquality.Ozone.mean()
    # Replace all the missing values in the Ozone column with the mean
    airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)
    或者可以dataframe替换
    airquality[['Ozone','xxxxx']] = airquality[['Ozone','xxxxx']].fillna(oz_mean)
    
    判断是否有空值
    In the video, you saw Dan use the .all() method together with the .notnull() DataFrame method to check for missing values in a column. 
    The .all() method returns True if all values are True. When used on a DataFrame, it returns a Series of Booleans - one for each column in the DataFrame. So if you are using it on a DataFrame, like in this exercise, 
    you need to chain another .all() method so that you return only one True or False value.
    
    Use the pd.notnull() function on ebola (or the .notnull() method of ebola) and chain two .all() methods 
    (that is, .all().all()). The first .all() method will return a True or False for each column, while the second .all() 
    method will return a single True or False.
    
    # Assert that there are no missing values
    assert pd.notnull(ebola).all().all()

    # Assert that all values are >= 0
    assert (ebola >= 0).all().all()
    
    ----------------------整体实例
    The DataFrame g1800s has been pre-loaded. Your job in this exercise is to create a scatter plot with life expectancy in '1800' on the x-axis and life expectancy in '1899' on the y-axis.
    Here, the goal is to visually check the data for insights as well as errors. When looking at the plot, pay attention to whether the scatter plot takes the form of a diagonal line, 
    and which points fall below or above the diagonal line. This will inform how life expectancy in 1899 changed (or did not change) 
    compared to 1800 for different countries. If points fall on a diagonal line, it means that life expectancy remained the same!
    
    import matplotlib.pyplot as plt
    # Create the scatter plot
    g1800s.plot(kind='scatter', x='1800', y='1899')
    # Specify axis labels
    plt.xlabel('Life Expectancy by Country in 1800')
    plt.ylabel('Life Expectancy by Country in 1899')
    # Specify axis limits
    plt.xlim(20, 55)
    plt.ylim(20, 55)
    # Display the plot
    plt.show()
    
    #处理NA数据
    def check_null_or_valid(row_data):
    """Function that takes a row of data,
    drops all missing values,
    and checks if all remaining values are greater than or equal to 0
    """
    no_na = row_data.dropna()
    numeric = pd.to_numeric(no_na)
    ge0 = numeric >= 0
    return ge0
    # Check whether the first column is 'Life expectancy'
    assert g1800s.columns[0] == 'Life expectancy'
    # Check whether the values in the row are valid
    assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()
    # Check that there is only one instance of each country
    assert g1800s['Life expectancy'].value_counts()[0] == 1
    
    #做的一个简单的例子
    import numpy as np
    data = {
         'state': [np.nan, 6, 7, 8, 9],
         'year': [11, np.nan, 12, 13, 14],
         'pop': [21, 22, np.nan, 23, 24]
    }

    def test(row):
        no_na =  row.dropna()
        res = no_na > 2
        return res

    df = pd.DataFrame(data)
    # print(df)
    df.apply(test,axis=1)
    print(df.all().all())
    
    #通过融合各列后并且修改columns
    import pandas as pd
    # Melt gapminder: gapminder_melt
    gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')
    # Rename the columns
    gapminder_melt.columns = ['country', 'year', 'life_expectancy']
    # Print the head of gapminder_melt
    print(gapminder_melt.head())

    #判断dtypes
    # Convert the year column to numeric
    gapminder.year = pd.to_numeric(gapminder.year)
    # Test if country is of type object
    assert gapminder.country.dtypes == np.object
    # Test if year is of type int64
    assert gapminder.year.dtypes == np.int64
    # Test if life_expectancy is of type float64
    assert gapminder.life_expectancy.dtypes == np.float64
    
    
    
==================================================================================
层次化索引
一、层次化索引
层次化索引(hierarchical indexing)是pandas的一个重要的功能，它可以在一个轴上有多个（两个以上）的索引，这就表示着，它能够以低维度形式来表示高维度的数据。

二、Series的层次化索引
    # Series的层次化索引，索引是一个二维数组，相当于两个索引决定一个值
    # 有点类似于DataFrame的行索引和列索引
    s = Series(np.arange(1,10),index=[["a","a","a","b","b","c","c","d","d"],
                                      [1,2,3,1,2,3,1,2,3]])
    print(s)
    '''
    a  1    1
       2    2
       3    3
    b  1    4
       2    5
    c  3    6
       1    7
    d  2    8
       3    9
    '''
    #显示层次化索引
    print(s.index)
    '''
    MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],
           labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 2]])
    '''
    #选取第一个索引为a的数据
    print(s["a"])
    '''
    1    1
    2    2
    3    3
    '''
    #层次化索引的切片，包括右端的索引
    print(s["c":"d"])#等价于s.ix[["c","d"]]
    '''
    c  3    6
       1    7
    d  2    8
       3    9
    '''
    print(type(s))
    #<class 'pandas.core.series.Series'>
    print(type(s.unstack()))
    #<class 'pandas.core.frame.DataFrame'>
    #通过unstack方法可以将Series变成一个DataFrame
    #数据的类型以及数据的输出结构都变成了DataFrame，对于不存在的位置使用NaN填充
    print(s.unstack())
    '''
         1    2    3
    a  1.0  2.0  3.0
    b  4.0  5.0  NaN
    c  7.0  NaN  6.0
    d  NaN  8.0  9.0
    '''
    #通过stack方法，可以将DataFrame变成Series，它是unstack的逆运算
    print(type(s.unstack().stack()))
    #<class 'pandas.core.series.Series'>
    print(s.unstack().stack())
    '''
    a  1    1.0
       2    2.0
       3    3.0
    b  1    4.0
       2    5.0
    c  1    7.0
       3    6.0
    d  2    8.0
       3    9.0
    '''
二、DataFrame的层次化索引
对于DataFrame来说，行和列都能够进行层次化索引。

    #DataFrame的行和列都是用层次化索引
    #也就是四个索引来决定一个值，将一个二维数据变成了一个四维数据
    data = DataFrame(np.arange(12).reshape(4,3),
                     index=[["a","a","b","b"],[1,2,1,2]],
                     columns=[["A","A","B"],["Z","X","C"]])
    print(data)
    '''
         A       B
         Z   X   C
    a 1  0   1   2
      2  3   4   5
    b 1  6   7   8
      2  9  10  11
    '''
    #选取列,参数只能选取"A"或"B"
    print(data["A"])#等价于data.ix[:,"A"]
    '''
         Z   X
    a 1  0   1
      2  3   4
    b 1  6   7
      2  9  10
    '''
    #选取行
    print(data.ix["a"])
    '''
       A     B
       Z  X  C
    1  0  1  2
    2  3  4  5
    '''
三、重排分级顺序
在使用层次化索引的时候，我们可以重新调整某条轴上各级别的顺序，或根据级别上的值对数据进行排序。swaplevel接受两个级别编号或名称，返回一个互换了级别的新对象（数据不变）。我们可以对每个级别设置一个名称，就像对DataFrame设置行列索引的名称一样。

    data = DataFrame(np.arange(12).reshape(4,3),
                     index=[["a","a","b","b"],[1,2,1,2]],
                     columns=[["A","A","B"],["Z","X","C"]])
    print(data)
    '''
         A       B
         Z   X   C
    a 1  0   1   2
      2  3   4   5
    b 1  6   7   8
      2  9  10  11
    '''
    #对每一个级别设置一个名称
    #设置行级别的名称
    data.index.names=["row1","row2"]
    #设置列级别的名称
    data.columns.names=["column1","column2"]
    print(data)
    '''
    column1    A       B
    column2    Z   X   C
    row1 row2
    a    1     0   1   2
         2     3   4   5
    b    1     6   7   8
         2     9  10  11
    '''
    #通过swaplevel，调整行的顺序
    print(data.swaplevel("row1","row2"))
    '''
    column1    A       B
    column2    Z   X   C
    row2 row1
    1    a     0   1   2
    2    a     3   4   5
    1    b     6   7   8
    2    b     9  10  11
    '''
    #对指定级别中的值进行排序
    #对级别名称为row2中的值进行排序
    print(data.sortlevel(1))#等价于data.swaplevel(0,1).sortlevel(0)
    '''
    column1    A       B
    column2    Z   X   C
    row1 row2           
    a    1     0   1   2
    b    1     6   7   8
    a    2     3   4   5
    b    2     9  10  11
    '''
四、根据级别汇总统计
    data = DataFrame(np.arange(12).reshape(4,3),
                     index=[["a","a","b","b"],[1,2,1,2]],
                     columns=[["A","A","B"],["Z","X","C"]])
    print(data)
    '''
         A       B
         Z   X   C
    a 1  0   1   2
      2  3   4   5
    b 1  6   7   8
      2  9  10  11
    '''
    #对每一个级别设置一个名称
    #设置行级别的名称
    data.index.names=["row1","row2"]
    #设置列级别的名称
    data.columns.names=["column1","column2"]
    print(data)
    '''
    column1    A       B
    column2    Z   X   C
    row1 row2
    a    1     0   1   2
         2     3   4   5
    b    1     6   7   8
         2     9  10  11
    '''
    #指定行级别的名称进行求和
    print(data.sum(level="row1"))
    '''
    column1   A       B
    column2   Z   X   C
    row1
    a         3   5   7
    b        15  17  19
    '''
    #指定列级别的名称进求和
    print(data.sum(level="column1",axis=1))
    '''
    column1     A   B
    row1 row2        
    a    1      1   2
         2      7   5
    b    1     13   8
         2     19  11

