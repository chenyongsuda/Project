1.初始化
Spark 2.0以前版本:
val sparkConf = new SparkConf().setAppName("soyo")
    val spark = new SparkContext(sparkConf)

Spark 2.0以后版本：（上面的写法兼容）
直接用SparkSession：
val spark = SparkSession
      .builder
      .appName("soyo")
      .getOrCreate()
    var tc = spark.sparkContext.parallelize(数据).cache()
    
2.V2.0读取语句
val file=spark.sparkContext.textFile("file:///home/soyo/桌面/spark编程测试数据/1.txt")
    val word=file.flatMap(lines=>lines.split(" ")).map(word=>(word,1)).reduceByKey(_+_)
    
读取CSV 创建DataFrame

方法一：用pandas辅助
from pyspark import SparkContext 
from pyspark.sql import SQLContext 
import pandas as pd 
sc = SparkContext()
sqlContext=SQLContext(sc) 
df=pd.read_csv(r'game-clicks.csv') 
sdf=sqlc.createDataFrame(df) 

方法二：纯spark
val spark = SparkSession.builder.config(conf).getOrCreate()
val dataFrame = spark.read.format("CSV").option("header","true").load(csvfilePath)

Dataframe 使用SQL必须要registerTempTable
df_alllist.registerTempTable("items")
然后使用
spark.sql("slecet count(*) from items")
SQL函数返回RDD



RDD是无模式的数据结构（不像DataFrames）。因此，在使用RDD时，并行化数据集对于Spark来说是完美的。
>>> data_heterogenous = sc.parallelize([('Ferrari','fast'),{'Porsche':100000},['Spain','visited',4504]]).collect()
所以，我们可以混合几乎任何东西：一个元组，一个字典，或一个列表。 
一旦你.collect（）数据集（即，运行一个动作将其返回给驱动程序），你可以像在Python中通常那样访问对象中的数据：

>>> data_heterogenous[1]['Porsche']
100000
1
2
.collect（）方法将RDD的所有元素返回到驱动程序，并将其作为列表序列化。



Pandas和Spark的DataFrame两者互相转换：
pandas_df = spark_df.toPandas()	spark_df = sqlContext.createDataFrame(pandas_df)





