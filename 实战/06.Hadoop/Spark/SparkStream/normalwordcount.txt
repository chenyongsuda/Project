#coding=utf-8

from pyspark import SparkContext,SparkConf
from pyspark.streaming import StreamingContext
from operator import add


if __name__ == "__main__":
    # 192.168.8.128 9999
    conf = SparkConf()
    conf.setAppName("sparkstreaming")
    sc = SparkContext(conf=conf)
    ssc = StreamingContext(sc, 10)

    sm = ssc.socketTextStream("192.168.8.128", 9999)
    words = sm.flatMap(lambda line: line.split(" "))
    word_pair = words.map(lambda word: (word, 1))
    result = word_pair.reduceByKey(add)
    result.pprint()

    ssc.start()
    ssc.awaitTermination()
    ssc.stop()



1 创建StreamingContext对象
首先使用StreamingContext模块，这个模块的作用是提供所有的流数据处理的功能：

复制代码
1 from pyspark import SparkContext
2 from pyspark.streaming import StreamingContext
3 
4 sc = SparkContext("local[2]", "streamwordcount")
5 # 创建本地的SparkContext对象，包含2个执行线程
6 
7 ssc = StreamingContext(sc, 2)
8 # 创建本地的StreamingContext对象，处理的时间片间隔时间，设置为2s
复制代码
2 创建DStream对象
我们需要连接一个打开的 TCP 服务端口，从而获取流数据，这里使用的源是TCP Socket，所以使用socketTextStream()函数：

lines = ssc.socketTextStream("localhost", 8888)
# 创建DStream，指明数据源为socket：来自localhost本机的8888端口
3 对DStream进行操作
我们开始对lines进行处理，首先对当前2秒内获取的数据进行分割并执行标准的MapReduce流程计算。

words = lines.flatMap(lambda line: line.split(" "))
# 使用flatMap和Split对2秒内收到的字符串进行分割
得到的words是一系列的单词，再执行下面的操作：

pairs = words.map(lambda word: (word, 1))
# map操作将独立的单词映射到(word，1)元组

wordCounts = pairs.reduceByKey(lambda x, y: x + y)
# reduceByKey操作对pairs执行reduce操作获得（单词，词频）元组
5 输出数据
将处理后的数据输出到一个文件中：

outputFile = "/home/feige/streaming/ss"
# 输出文件夹的前缀，Spark Streaming会自动使用当前时间戳来生成不同的文件夹名称

wordCounts.saveAsTextFiles(outputFile)
# 将结果输出
6 启动应用
要使程序在Spark Streaming上运行起来，需要执行Spark Streaming启动的流程，调用start()函数启动，awaitTermination()函数等待处理结束的信号。

ssc.start() 
# 启动Spark Streaming应用
ssc.awaitTermination()
打开终端执行：

nc -lk 8888
nc的-l参数表示创建一个监听端口，等待新的连接。-k参数表示当前连接结束后仍然保持监听，必须与-l参数同时使用。

执行完上面的命令后不关闭终端，我们将在这个终端中输入一些处理的数据:
