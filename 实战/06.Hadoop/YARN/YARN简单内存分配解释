mapreduce on yarn简单内存分配解释
关于mapreduce程序运行在yarn上时内存的分配一直是一个让我蒙圈的事情，单独查任何一个资料都不能很好的理解透彻。于是，最近查了大量的资料，综合各种解释，终于理解到了一个比较清晰的程度，在这里将理解的东西做一个简单的记录，以备忘却。
首先，先将关于mapreduce和yarn关于内存分配的参数粘贴上:
yarn.scheduler.minimum-allocation-mb
yarn.scheduler.maximum-allocation-mb
yarn.nodemanager.resource.memory-mb
yarn.nodemanager.vmem-pmem-ratio
yarn.scheduler.increment-allocation-mb
mapreduce.map.memory.mb
mapreduce.reduce.memory.mb
mapreduce.map.java.opts
mapreduce.reduce.java.opts
个人认为，针对mapreduce任务，这些参数只有放在一起学习才能真正理解，如果单独考虑，理解不清晰。下面开始详细讲解。
一、理解参数yarn.nodemanager.resource.memory-mb,yarn.nodemanager.vmem-pmem-ratio
yarn.nodemanager.resource.memory-mb很简单，就是你的这台服务器节点上准备分给yarn的内存;
yarn.nodemanager.vmem-pmem-ratio网上解释都是"每使用1MB物理内存，最多可用的虚拟内存数，默认2.1"，但是目前我还是不太理解其作用是什么，有知道的朋友希望能详细解释下。
二、理解参数yarn.scheduler.minimum-allocation-mb和yarn.scheduler.maximum-allocation-mb
都知道，在yarn上运行程序时每个task都是在独立的Container中运行的，单个Container可以申请的最小和最大内存的限制就是这两个参数，注意，并不是这两个参数决定单个Container申请内存的大小，而仅仅是限制的一个范围。
三、理解yarn的内存规整化因子和内存规整化算法
先不说和哪个参数有关，单纯理解这一概念。举例:
假如规整化因子b=512M，上述讲的参数yarn.scheduler.minimum-allocation-mb为1024，yarn.scheduler.maximum-allocation-mb为8096，然后我打算给单个map任务申请内存资源(mapreduce.map.memory.mb):
申请的资源为a=1000M时，实际得到的Container内存大小为1024M(小于yarn.scheduler.minimum-allocation-mb的话自动设置为yarn.scheduler.minimum-allocation-mb);
申请的资源为a=1500M时，实际得到的Container内存大小为1536M，计算公式为:ceiling(a/b)*b，即ceiling(a/b)=ceiling(1500/512)=3,3*512=1536。此处假如b=1024，则Container实际内存大小为2048M
也就是说Container实际内存大小最小为yarn.scheduler.minimum-allocation-mb值，然后增加时的最小增加量为规整化因子b，最大不超过yarn.scheduler.maximum-allocation-mb
四、理解mapreduce.map.memory.mb、mapreduce.reduce.memory.mb
"三"中提到的"打算给单个map任务申请内存资源"也就是a,其实就是指的"mapreduce.map.memory.mb"或"mapreduce.reduce.memory.mb"，注意其值不要超过yarn.scheduler.maximum-allocation-mb
五、理解mapreduce.map.java.opts、mapreduce.reduce.java.opts
以map任务为例，Container其实就是在执行一个脚本文件，而脚本文件中，会执行一个 Java 的子进程，这个子进程就是真正的 Map Task，mapreduce.map.java.opts 其实就是启动 JVM 虚拟机时，传递给虚拟机的启动参数，而默认值 -Xmx200m 表示这个 Java 程序可以使用的最大堆内存数，一旦超过这个大小，JVM 就会抛出 Out of Memory 异常，并终止进程。而 mapreduce.map.memory.mb 设置的是 Container 的内存上限，这个参数由 NodeManager 读取并进行控制，当 Container 的内存大小超过了这个参数值，NodeManager 会负责 kill 掉 Container。在后面分析 yarn.nodemanager.vmem-pmem-ratio 这个参数的时候，会讲解 NodeManager 监控 Container 内存（包括虚拟内存和物理内存）及 kill 掉 Container 的过程。
也就是说，mapreduce.map.java.opts一定要小于mapreduce.map.memory.mb
mapreduce.reduce.java.opts同mapreduce.map.java.opts一样的道理。
六、理解规整化因子指的是哪个参数
"三"中提到的规整化因子也就是b，具体指的是哪个参数和yarn使用的调度器有关，一共有三种调度器:capacity scheduler（默认调度器）、fair scheduler和fifo scheduler
当使用capacity scheduler或者fifo scheduler时，规整化因子指的就是参数yarn.scheduler.minimum-allocation-mb，不能单独配置，即yarn.scheduler.increment-allocation-mb无作用;
当使用fair scheduler时，规整化因子指的是参数yarn.scheduler.increment-allocation-mb
至此，关于yarn和mapreduce的任务内存配置问题讲完了，这也是我目前理解的层次。

在hadoop2及以上版本中，map和reduce task 是运行在container中的。mapreduce.{map|reduce}.memory.mb 
被yarn用来设置container的内存大小。如果container的内存超限，会被yarn杀死。在container中，为了执行map和reduce task，
yarn会在contaner中启动一个jvm来执行task任务。mapreduce.{map|reduce}.java.opts用来设置container启动的jvm相关参数，
通过设置Xmx来设置map 或者reduce task的最大堆内存。 
理论上，{map|reduce}.java.opts设置的最大堆内存要比{map|reduce}.memory.mb小。一般设置为一般设置为0.75倍的memory.mb即可；
因为在yarn container这种模式下，JVM进程跑在container中，需要为java code等非JVM的内存使用预留些空间。 

内存分配增量/规整因子/incrementMemory
为了易于管理资源和调度资源，Hadoop YARN内置了资源规整化算法，它规定了最小可申请资源量、最大可申请资源量和资源规整化因子，规整化因子是用来规整化应用程序资源的，应用程序申请的资源如果不是该因子的整数倍，则将被修改为最小的整数倍对应的值，公式为ceil(a/b)*b，其中a是应用程序申请的资源，b为规整化因子。对于规整化因子，不同调度器不同，具体如下： 
FIFO和Capacity Scheduler，规整化因子等于最小可申请资源量，不可单独配置。 
Fair Scheduler：规整化因子通过参数yarn.scheduler.increment-allocation-mb和yarn.scheduler.increment-allocation-vcores设置，默认是1024和1。 
通过以上介绍可知，应用程序申请到资源量可能大于资源申请的资源量，比如YARN的最小可申请资源内存量为1024，规整因子是1024，
如果一个应用程序申请1500内存，则会得到2048内存，如果规整因子是512，则得到1536内存。
对于56G内存的NM来说，如果全部跑map则56/3大约跑18个container 
大概了解完以上的参数之后，mapreduce.map.java.opts和mapreduce.map.memory.mb参数之间，有什么联系呢？ 
通过上面的分析，我们知道如果一个yarn的container超除了heap设置的大小，这个task将会失败，
我们可以根据哪种类型的container失败去相应增大mapreduce.{map|reduce}.memory.mb去解决问题。 
但同时带来的问题是集群并行跑的container的数量少了，所以适当的调整内存参数对集群的利用率的提升尤为重要。


如下图：map container的内存(“mapreduce.map.memory.mb”)被设置为1536mb 。但是AM会为其申请了2048m的内存，因为am的最小分配单位/增量（yarn.scheduler.minimum-allocation-mb）被设置为1024，也就是以1GB为单位往上加。这是一种逻辑上的分配，这个值被NodeManager用来监控该进程内存资源的使用率，如果mapTask的堆内存使用率超过了2048MB，NM将会把这个task给杀掉。



Each machine in our cluster has 48 GB of RAM. Some of this RAM should be >reserved for Operating System usage. On each node, we’ll assign 40 GB RAM for >YARN to use and keep 8 GB for the Operating System

For our example cluster, we have the minimum RAM for a Container (yarn.scheduler.minimum-allocation-mb) = 2 GB. We’ll thus assign 4 GB for Map task Containers, and 8 GB for Reduce tasks Containers.

In mapred-site.xml:
mapreduce.map.memory.mb: 4096
mapreduce.reduce.memory.mb: 8192

Each Container will run JVMs for the Map and Reduce tasks. The JVM heap size should be set to lower than the Map and Reduce memory defined above, so that they are within the bounds of the Container memory allocated by YARN.
In mapred-site.xml:
mapreduce.map.java.opts: -Xmx3072m
mapreduce.reduce.java.opts: -Xmx6144m

The above settings configure the upper limit of the physical RAM that Map and Reduce tasks will use.
To sum it up:
In YARN, you should use the mapreduce configs, not the mapred ones. EDIT: This comment is not applicable anymore now that you've edited your question.
What you are configuring is actually how much you want to request, not what is the max to allocate.
The max limits are configured with the java.opts settings listed above.
Finally, you may want to check this other SO question that describes a similar problem (and solution).


yarn.nodemanager.resource.memory-mb默认值为-1，代表着YARN的NodeManager占总内存的80%。也就是说加入我们的机器为64GB内存，出去非YARN进程需要的20%内存，我们大概需要64*0.8≈51GB，在分配的时候，单个任务可以申请的默认最小内存为1G，任务量大的话可最大提高到8GB。
    在生产场景中，简单的配置，一般情况下：yarn.nodemanager.resource.memory-mb直接设置成我们需要的值，且要是最大和最小内存需求的整数倍；（一般Container容器中最小内存为4G，最大内存为16G）

    假如：64GB的机器内存，我们有51GB的内存可用于NodeManager分配，根据上面的介绍，我们可以直接将yarn.nodemanager.resource.memory-mb值为48GB，然后容器最小内存为4GB，最大内存为16GB，也就是在当前的NodeManager节点下，我们最多可以有12个容器，最少可以有3个容器。

    CPU配置

    此处的CPU指的是虚拟的CPU（CPU virtual core），之所以产生虚拟CPU（CPU vCore）这一概念，是因为物理CPU的处理能力的差异，为平衡这种差异，就引入这一概念。

    yarn.nodemanager.resource.cpu-vcores表示能够分配给Container的CPU核数，默认配置为-1，代表值为8个虚拟CPU，推荐该值的设置和物理CPU的核数数量相同，若不够，则需要调小该值。

    yarn.scheduler.minimum-allocation-vcores的默认值为1，表示每个Container容器在处理任务的时候可申请的最少CPU个数为1个。

    yarn.scheduler.maximum-allocation-vcores的默认值为4，表示每个Container容器在处理任务的时候可申请的最大CPU个数为4个。

PS：以上内容是小编我学习的理解，以上所示的参数为配置过程中基本的参数，如果想配置更为合理，资源利用率更高，可参考小编转载的两篇文章，初学者，如有错误请指出，谢谢。


    mapreduce.map.memory.mb = (1~2倍) * yarn.scheduler.minimum-allocation-mb
　　mapreduce.reduce.memory.mb = (1~4倍) * yarn.scheduler.minimum-allocation-mb

　　1. mapred.child.java.opts = -XmxTm(T数字要小于map和reduce的设置value)
　　2. mapreduce.map.java.opts=-Xmx(<mapreduce.map.memory.mb)m
　　   mapreduce.reduce.java.opts=-Xmx(<mapreduce.reduce.memory.mb)m

　　总结：最终运行参数给定的jvm堆大小必须小于参数指定的map和reduce的memory大小，最好为70%以下。
