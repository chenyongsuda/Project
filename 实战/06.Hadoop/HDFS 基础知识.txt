HDFS
    1.  主要角色
        Name Node           存储元数据快照和变更日志
        Data Node           存储真实数据的节点
        Second NameNode     辅助合并变更日志生成最新快照的辅助节点

        Secondary NameNode:它究竟有什么作用？
            在Hadoop中，有一些命名不好的模块，Secondary NameNode是其中之一。从它的名字上看，它给人的感觉就像是NameNode的备份。但它实际上却不是。
            很多Hadoop的初学者都很疑惑，Secondary NameNode究竟是做什么的，而且它为什么会出现在HDFS中。因此，在这篇文章中，我想要解释下Secondary NameNode在HDFS中所扮演的角色。
            从它的名字来看，你可能认为它跟NameNode有点关系。没错，你猜对了。因此在我们深入了解Secondary NameNode之前，我们先来看看NameNode是做什么的。

            NameNode
            NameNode主要是用来保存HDFS的元数据信息，比如命名空间信息，块信息等。当它运行的时候，这些信息是存在内存中的。但是这些信息也可以持久化到磁盘上。
            fsimage - 它是在NameNode启动时对整个文件系统的快照
            edit logs - 它是在NameNode启动后，对文件系统的改动序列
            只有在NameNode重启时，edit logs才会合并到fsimage文件中，从而得到一个文件系统的最新快照。但是在产品集群中NameNode是很少重启的，这也意味着当NameNode运行了很长时间后，edit logs文件会变得很大。在这种情况下就会出现下面一些问题：

            edit logs文件会变的很大，怎么去管理这个文件是一个挑战。
            NameNode的重启会花费很长时间，因为有很多改动[笔者注:在edit logs中]要合并到fsimage文件上。
            如果NameNode挂掉了，那我们就丢失了很多改动因为此时的fsimage文件非常旧。[笔者注: 笔者认为在这个情况下丢失的改动不会很多, 因为丢失的改动应该是还在内存中但是没有写到edit logs的这部分。]
            因此为了克服这个问题，我们需要一个易于管理的机制来帮助我们减小edit logs文件的大小和得到一个最新的fsimage文件，这样也会减小在NameNode上的压力。这跟Windows的恢复点是非常像的，Windows的恢复点机制允许我们对OS进行快照，这样当系统发生问题时，我们能够回滚到最新的一次恢复点上。

            现在我们明白了NameNode的功能和所面临的挑战 - 保持文件系统最新的元数据。那么，这些跟Secondary NameNode又有什么关系呢？
            Secondary NameNode
                SecondaryNameNode就是来帮助解决上述问题的，它的职责是合并NameNode的edit logs到fsimage文件中。
                首先，它定时到NameNode去获取edit logs，并更新到fsimage上。[笔者注：Secondary NameNode自己的fsimage]
                一旦它有了新的fsimage文件，它将其拷贝回NameNode中。
                NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。
                Secondary NameNode的整个目的是在HDFS中提供一个检查点。它只是NameNode的一个助手节点。这也是它在社区内被认为是检查点节点的原因。

                现在，我们明白了Secondary NameNode所做的不过是在文件系统中设置一个检查点来帮助NameNode更好的工作。它不是要取代掉NameNode也不是NameNode的备份。
                所以从现在起，让我们养成一个习惯，称呼它为检查点节点吧。
            这篇文章基本上已经清楚的介绍了Secondary NameNode的工作以及为什么要这么做。最后补充一点细节，
            是关于NameNode是什么时候将改动写到edit logs中的？这个操作实际上是由DataNode的写操作触发的，当我们往DataNode写文件时，
            DataNode会跟NameNode通信，告诉NameNode什么文件的第几个block放在它那里，NameNode这个时候会将这些元数据信息写到edit logs文件中。
    
    2.对外Web
        主要对外web一般都是开在namenode机器上的50070端口上的
    
    3.HDFS命令
        hdfs fsck 
            -move: 移动损坏的文件到/lost+found目录下 
            -delete: 删除损坏的文件 
            -openforwrite: 输出检测中的正在被写的文件 
            -list-corruptfileblocks: 输出损坏的块及其所属的文件 
            -files: 输出正在被检测的文件 
            -blocks: 输出block的详细报告 （需要和-files参数一起使用） 
            -locations: 输出block的位置信息 （需要和-files参数一起使用） 
            -racks: 输出文件块位置所在的机架信息（需要和-files参数一起使用） 

        举例：
            hadoop fsck /your_file_path -files -blocks -locations -racks 
            或者
            hdfs fsck /your_file_path -files -blocks -locations -racks 
            查看文件blocks location racks
                Status: HEALTHY
                Total size:    70 B                                             总size
                Total dirs:    0                                                总dir
                Total files:   1                                                总文件数
                Total symlinks:                0                                总链接
                Total blocks (validated):      1 (avg. block size 70 B)         总block数
                ------------------------------------------------------------------------------
                Minimally replicated blocks:   1 (100.0 %)                      
                Over-replicated blocks:        0 (0.0 %)
                Under-replicated blocks:       0 (0.0 %)
                Mis-replicated blocks:         0 (0.0 %)                        
                ------------------------------------------------------------------------------
                Default replication factor:    3                                默认负载因子
                Average block replication:     3.0                              平均block副本
                Corrupt blocks:                0                                损坏的blocks    (3份全丢后，状态就为corrupt)
                Missing replicas:              0 (0.0 %)                        丢失的blocks
                -------------------------------------------------------------------------------
                Number of data-nodes:          10                               总共10个DataNode
                Number of racks:               1                                总共1个机架
                FSCK ended at Wed Aug 10 17:17:45 CST 2016 in 1 milliseconds
            打印文件的Block报告（-blocks）
                [hadoop@dev ~]$  hdfs fsck /logs/site/2015-08-08/lxw1234.log -files -blocks
                FSCK started by hadoop (auth:SIMPLE) from /172.16.212.17 for path /logs/site/2015-08-08/lxw1234.log at Thu Aug 13 09:45:59 CST 2015
                /logs/site/2015-08-08/lxw1234.log 7408754725 bytes, 56 block(s):  OK
                0. BP-1034052771-172.16.212.130-1405595752491:blk_1075892982_2152381 len=134217728 repl=2
                1. BP-1034052771-172.16.212.130-1405595752491:blk_1075892983_2152382 len=134217728 repl=2
                2. BP-1034052771-172.16.212.130-1405595752491:blk_1075892984_2152383 len=134217728 repl=2
                ...
                其中，/logs/site/2015-08-08/lxw1234.log 7408754725 bytes, 56 block(s): 表示文件的总大小和block数；
                前面的0. 1. 2.代表该文件的block索引，56的文件块，就从0-55;
                BP-1034052771-172.16.212.130-1405595752491:blk_1075892982_2152381表示block id；
                len=134217728 表示该文件块大小；
                repl=2 表示该文件块副本数；
            查看文件中损坏的块（-list-corruptfileblocks）
                [hadoop@dev ~]$ hdfs fsck /hivedata/warehouse/liuxiaowen.db/lxw_product_names/ -list-corruptfileblocks
            打印文件块的位置信息（-locations）
                [hadoop@dev ~]$  hdfs fsck /logs/site/2015-08-08/lxw1234.log -files -blocks -locations
                /logs/site/2015-08-08/lxw1234.log 7408754725 bytes, 56 block(s):  OK
                0. BP-1034052771-172.16.212.130-1405595752491:blk_1075892982_2152381 len=134217728 repl=2 [172.16.212.139:50010, 172.16.212.135:50010]
                1. BP-1034052771-172.16.212.130-1405595752491:blk_1075892983_2152382 len=134217728 repl=2 [172.16.212.140:50010, 172.16.212.133:50010]
                2. BP-1034052771-172.16.212.130-1405595752491:blk_1075892984_2152383 len=134217728 repl=2 [172.16.212.136:50010, 172.16.212.141:50010]
                和打印出的文件块信息相比，多了一个文件块的位置信息：[172.16.212.139:50010, 172.16.212.135:50010]
            打印文件块位置所在的机架信息（-racks）
                [hadoop@dev ~]$  hdfs fsck /logs/site/2015-08-08/lxw1234.log -files -blocks -locations -racks
                0. BP-1034052771-172.16.212.130-1405595752491:blk_1075892982_2152381 len=134217728 repl=2 [/default-rack/172.16.212.139:50010, /default-rack/172.16.212.135:50010]
                1. BP-1034052771-172.16.212.130-1405595752491:blk_1075892983_2152382 len=134217728 repl=2 [/default-rack/172.16.212.140:50010, /default-rack/172.16.212.133:50010]
                2. BP-1034052771-172.16.212.130-1405595752491:blk_1075892984_2152383 len=134217728 repl=2 [/default-rack/172.16.212.136:50010, /default-rack/172.16.212.141:50010]
                和前面打印出的信息相比，多了机架信息：[/default-rack/172.16.212.139:50010, /default-rack/172.16.212.135:50010]

        元数据获取方式：
        NameNode作为HDFS中文件目录和文件分配的管理者，它保存的最重要信息，就是下面两个映射：
        文件名=>数据块
        数据块=>DataNode列表

        其中，文件名=>数据块保存在磁盘上（持久化）；但NameNode上不保存数据块=>DataNode列表，该列表是通过DataNode上报建立起来的。
        