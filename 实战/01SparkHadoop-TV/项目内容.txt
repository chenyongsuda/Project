SparkHadoop项目
1.安装虚拟机配置网络
修改
BOOTPROTO=static  #设置静态Ip
ONBOOT=yes  #这里如果为no的话就改为yes，表示网卡设备自动启动
增加
GATEWAY=192.168.10.2  #这里的网关地址就是第二步获取到的那个网关地址
IPADDR=192.168.10.150  #配置ip，在第二步已经设置ip处于192.168.10.xxx这个范围，我就随便设为150了，只要不和网关相同均可
NETMASK=255.255.255.0#子网掩码
DNS1=8.8.8.8#dns服务器1，填写你所在的网络可用的dns服务器地址即可

重启生效
service network restart
（这部分还没搞明白先忽略使用动态地址）

2.安装三台虚拟机
VM01 192.168.8.128
VM02 192.168.8.129
VM03 192.168.8.130

修改域名


3.安装hadoop

卸载OpenJDK 
rpm -qa | grep jdk | xargs rpm -e --nodeps  
安装JDK
###############JDK
export JAVA_HOME=/appl/jdk
export PATH=$PATH:$JAVA_HOME/bin

下载hadoop-2.6.5.tar.gz
解压
配置/etc/profile增加

export HADOOP_HOME=/appl/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

测试是否成功
hadoop version
hadoop fs -ls /

4.复制JDK hadoop profile
scp -r /appl/jdk/  root@192.168.8.130:/appl/
ln -s hadoop-2.7.5/ hadoop
scp /etc/profile  root@192.168.8.130:/etc/profile

5.配置主机名对于ip  配置SSH 免密码
/etc/hosts 增加对于关系
修改主机名
/etc/hostname vm01
scp /etc/hosts root@vm03:/etc/hosts

生成秘钥 三个机器都生成下
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
拷贝秘钥
scp ~/.ssh/id_rsa.pub root@vm03:~/.ssh/authorized_keys

6.修改hadoop配置
core-site.xml
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://vm01/</value>
	    </property>
	</configuration>

hdfs-site.xml
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	   <name>dfs.replication</name>
	   <value>2</value>
	</property>
	</configuration>

	cp mapred-site.xml.template mapred-site.xml
	<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	       <name>mapreduce.framework.name</name>
	       <value>yarn</value>
	</property>
	</configuration>

nano yarn-site.xml
	<?xml version="1.0"?>
	<configuration>
	   <property>
	        <name>yarn.resourcemanager.hostname</name>
	        <value>vm01</value>
	   </property>
	   <property>
	        <name>yarn.nodemanager.aux-services</name>
	        <value>mapreduce_shuffle</value>
	   </property>
	</configuration>

slaves
	vm02
	vm03

nano hadoop-env.sh 
export JAVA_HOME=/appl/jdk


7. 分发hadoop配置
scp -r  hadoop/ root@vm03:/appl/hadoop/etc/

8.格式化文件
hadoop namenode -format

9.启动hadoop
start-all.sh

10.启动成功
[root@vm01 etc]# jps
1369 NameNode
1549 SecondaryNameNode
1694 ResourceManager
1951 Jps
启动成功

hadoopadmin 界面 http://192.168.8.128:50070/

第二部分
利用hadoop自带的demo来计算wordcount
mkdir input
   87  cd input/
   88  touch test01.txt
   89  echo 'hello world' >> test01.txt 
   90  echo 'hello hadoop' >> test02.txt 
   91  echo 'hello mapreduce' >> test02.txt
   93  hadoop fs -mkdir /wc_input
   94  hadoop fs -lsr /
   95  hadoop fs -put test* /wc_input


cd /appl/hadoop/share/hadoop/mapreduce/
运行jar包
hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wc_input /wc_output
运行完成
查看结果
hadoop fs cat /wc_output/part-r-00000

附加作业：写一个小教本如jps打印出三台机器的状态


附加：
防止时间不同步每台机器安装ntp
yum install ntp

然后通过ntpdate asia.pool.ntp.org

#crontab -e 
*/10 * * * *  /usr/sbin/ntpdate -u asia.pool.ntp.org >/dev/null 2>&1
#service crond restart


第三部分
Hive的工作原理：将文本数据与表结构数据做mapping,这样就可以通过SQL来查询.
将关联关系存在mysql中.
安装依赖库
搜索yum search libaio
yun install libaio

检查mysql是否安装
yum list installed |grep mysql

如果有卸载
yum remove -y xxx

下载仓库
wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm (centos7)
导入本地仓库
yum localinstall mysql-community-release-el7-5.noarch.rpm
验证本地仓库是否导入
yum repolist enabled | grep "mysql.*-community.*"
查看所有版本
yum repolist all | grep mysql
启用禁用某些版本
yum-config-manager --disable mysql56-community
yum-config-manager --enable mysql57-community-dmr

安装
yum install mysql-community-server 

启动mysql
systemctl start mysql (centos修改为了systemctl)
systemctl status mysql

进入mysql
mysql
>> show databases;
>> create database if not exists hive_metadata;

授权
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive'; 为hive：hive 来自所有机器的hive metadata表的权限
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive'; 为hive：hive 来自localhost机器的hive metadata表的权限
生效权限
flush privileges;

准备好环境后下载hive包随便选取一个版本2.1.2
解压后设置环境变量
###############HIVE
export HIVE_HOME=/appl/hive
export PATH=$PATH:$HIVE_HOME/bin
source /etc/profile

由于要连接mysql需要连接驱动下载mysql-connector-java-5.1.31.jar
放入hive目录下的lib文件夹中 拷贝完成设置配置文件

cp hive-env.sh.template hive-env.sh
cp hive-default.xml.template hive-site.xml
cp hive-log4j2.properties.template hive-log4j2.properties
cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties


解放hive-env.sh
mv hive-env.sh.template hive-env.sh
修改为
export JAVA_HOME=/appl/jdk
# Set HADOOP_HOME to point to a specific hadoop install directory
export HADOOP_HOME=/appl/hadoop       
# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/appl/hive/conf
# Folder containing extra ibraries required for hive compilation/execution can be controlled by:
export HIVE_AUX_JARS_PATH=/appl/hive/lib 

修改配置文件名为hive-site.xml 修改连接
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://192.168.8.128:3306/hive_metadata?createDatabaseIfNotExist=true</value>

<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>

<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>

<name>javax.jdo.option.ConnectionPassword</name>
<value>hive</value>

配置hive数据存储位置
新增三个存储文件夹由于hive依赖hdfs 所以要在hdfs建立存储文件夹
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/warehouse
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/tmp
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/log
hadoop fs -chmod -R 777 /hive-data/warehouse
hadoop fs -chmod -R 777 /hive-data/tmp
hadoop fs -chmod -R 777 /hive-data/log


<property>
    <name>hive.exec.scratchdir</name>
    <value>/hive-data/tmp</value>
</property>
HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。

<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/hive-data/warehouse</value>
</property>
Hive 默认的数据文件存储路径，通常为 HDFS 可写的路径。 

<property>
    <name>hive.querylog.location</name>
    <value>/hive-data/log</value>
</property>
Hive 实时查询日志所在的目录，如果该值为空，将不创建实时的查询日志。 通常为 HDFS 可写的路径

设置本地临时目录
mkdir /appl/hive-tmp/
把{system:java.io.tmpdir} 改成 /appl/hive-tmp/

把 {system:user.name} 改成 {user.name}

初始化元数据到mysql
./schematool -initSchema -dbType mysql
初始化完成


hive 进入命令行
create database tony;

use tont;

create table test (mykey string,myval string);

insert into test values("1","www.ymq.io");

select * from test; 查处结果表示成功

创建逗号分隔的表
create table t2(id int, name string, age string, tel string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

创建a.txt
1,chenyong,32
2,fangfa,28
3.chenwei,31
4.chenfangwei,32
5.beijun,28
6.xiaoxun,28

将本地数据和hive表结构关联

LOAD DATA LOCAL INPATH '/appl/a.txt' OVERWRITE INTO TABLE t2; 本地数据导入
LOAD DATA INPATH '/appl/a.txt' OVERWRITE INTO TABLE t2; HDFS数据导入


第四部分 Flume

Flume概念：
Flume可以直接到HDFS或者到kafka里面
源source(文件.文件夹),管道channel,目标sink(hdfs.kafka)

下载flume1.8
解压
做软链接
ln -s apache-flume-1.8.0-bin/ ./flume

添加环境变量
###############FLUME
export FLUME_HOME=/appl/flume
export PATH=$PATH:$FLUME_HOME/bin

source /etc/profile

flume-ng version 查看版本

修改flume的变量在conf/flume-env.sh
添加java环境变量
export JAVA_HOME=/appl/jdk/

创建监听配置文件在conf目录下创建一个net-logger-test.conf
写入内容
	# Name the components on this agent  
	a1.sources = r1  
	a1.sinks = k1  
	a1.channels = c1  
	  
	# Describe/configure the source  
	a1.sources.r1.type = netcat  
	a1.sources.r1.bind = localhost  
	a1.sources.r1.port = 44444  
	  
	# Describe the sink  
	a1.sinks.k1.type = logger  
	  
	# Use a channel which buffers events in memory  
	a1.channels.c1.type = memory  
	a1.channels.c1.capacity = 1000  
	a1.channels.c1.transactionCapacity = 100  
	  
	# Bind the source and sink to the channel  
	a1.sources.r1.channels = c1  
	a1.sinks.k1.channel = c1  

flume-ng agent -n a1 -c conf -f /appl/flume/conf/net-logger-test.conf -Dflume.root.logger=INFO,console
-n 表示名字
-c 制定配置文件所在目录
-f 制定采集方案文件

测试使用nc
nc localhost 44444
>> hello

实例源监听文件夹=>目标输出到HDFS
	#agent名， source、channel、sink的名称
	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1
	#具体定义source
	a1.sources.r1.type = spooldir
	a1.sources.r1.spoolDir = /appl/monitor
	#具体定义channel
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 10000
	a1.channels.c1.transactionCapacity = 100
	#具体定义sink
	a1.sinks.k1.type = hdfs
	a1.sinks.k1.hdfs.path = hdfs://vm01/weblogs/flume/%Y-%m-%d
	a1.sinks.k1.hdfs.filePrefix = access_log
	a1.sinks.k1.hdfs.fileType = DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp = true
	#不按照条数生成文件
	a1.sinks.k1.hdfs.rollCount = 0
	#HDFS上的文件达到128M时生成一个文件
	a1.sinks.k1.hdfs.rollSize = 134217728
	#HDFS上的文件达到60秒生成一个文件
	a1.sinks.k1.hdfs.rollInterval = 60
	 
	#组装source、channel、sink
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1


flume支持的source种类
常用的有
net
spooldir 监听目录将目录文件读取读完后设置为OK 不实时 支持断点
对指定目录进行实时监控，如发现目录新增文件，立刻收集并发送
缺点：不能对目录文件进行修改，如果有追加内容的文本文件，不允许，

exec 执行如tail -f 监听单个文件 比较实时但是不支持断点
taildirSource 实时支持文件夹中的匹配的所有文件的监控支持断点--->这个是最好的最实用的


taildir实例
	#agent名， source、channel、sink的名称
	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1
	#具体定义source
	a1.sources.r1.type = TAILDIR
	a1.sources.r1.channels = c1
	a1.sources.r1.positionFile = /appl/monitor/taildir_position.json  
	a1.sources.r1.filegroups = f1                          
	a1.sources.r1.filegroups.f1 = /appl/monitor/.*.log  
	a1.sources.r1.headers.f1.headerKey1 = value1             
	a1.sources.r1.fileHeader = true

	#具体定义channel
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 10000
	a1.channels.c1.transactionCapacity = 100
	#具体定义sink
	a1.sinks.k1.type = hdfs
	a1.sinks.k1.hdfs.path = hdfs://vm01/weblogs/flume/%Y-%m-%d
	a1.sinks.k1.hdfs.filePrefix = access_log
	a1.sinks.k1.hdfs.fileType = DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp = true
	#不按照条数生成文件
	a1.sinks.k1.hdfs.rollCount = 0
	#HDFS上的文件达到128M时生成一个文件
	a1.sinks.k1.hdfs.rollSize = 1024
	#HDFS上的文件达到60秒生成一个文件
	a1.sinks.k1.hdfs.rollInterval = 0
	 
	#组装source、channel、sink
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1

##########################################Hive外表说明######################
在hive中，外表是个很重要的组成部分，通过外表可以很方便进行数据的共享。
因为普通的表会将数据文件拷贝自己的目录下，这样想要分享数据只能保存多份数据。
但是外表很好的解决了这个问题。
CREATE EXTERNAL TABLE sunwg_test09(id INT, name string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ‘\t’
LOCATION ‘/sunwg/test08′;

上面的语句创建了一张名字为sunwg_test09的外表，该表有id和name两个字段，
字段的分割符为tab，文件的数据文件夹为/sunwg/test08

实际上外表不光可以指定hdfs的目录，本地的目录也是可以的。
比如：
CREATE EXTERNAL TABLE test10(id INT, name string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ‘\t’
LOCATION ‘file:////home/hjl/sunwg/’;

#######################################MapReduce和Hive支持文件夹递归##########
一般情况下，传递给MapReduce和Hive的input文件夹中不能包含子目录，否则就会报错。但后来增加了递归遍历Input目录的功能，
这个貌似是从0.23开始的，具体不清楚，反正在0.20中是不支持的。
MapReduce
默认情况下，mapreduce.input.fileinputformat.input.dir.recursive为flase.
设置mapreduce.input.fileinputformat.input.dir.recursive=true，这个参数是客户端参数，可以在MapReduce中设置，也可以在mapred-site.xml中设置，无所谓。

CREATE EXTERNAL TABLE lxw1234 (d string) stored AS textfile location '/tmp/lxw1234/';
查询：select * from lxw1234;
同样报错 “Not a file: hdfs://cdh5/tmp/lxw1234/subdir” 。

在hive-cli中设置参数：
set hive.mapred.supports.subdirectories=true;
set mapreduce.input.fileinputformat.input.dir.recursive=true;
也可以Hive脚本中添加

实例：

CREATE EXTERNAL TABLE tony.testweblog(id int, name string, age string, tel string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/weblogs/flume/';

这样就讲flume收集上来的数据和hive挂钩了
下面需要写个代码定时产生相关数据

实验脚本：
内容：定时差生如下格式数据
id int, name string, age string, tel string
1,chenyong,32,18721688888

脚本内容
#!/bin/bash  

MAX=10000

function rand(){    
    min=$1    
    max=$(($2-$min+1))    
    num=$(($RANDOM+1000000000)) #增加一个10位的数再求余    
    echo $(($num%$max+$min))    
}

for (( i = 0; i < MAX ; i ++ ))
do
	RAND_NUM=$(rand 1 6)
	if [ "$RAND_NUM" -eq "1" ];then
    	echo "$i,beijun,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "2" ];then
    	echo "$i,chenfangwei,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "3" ];then
    	echo "$i,chenwei,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "4" ];then
    	echo "$i,chenyong,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "5" ];then
    	echo "$i,fangfa,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "6" ];then
    	echo "$i,xiaoxun,32,123" >> /appl/monitor/1.log
    fi
    sleep 1s
done

==================hive的特殊设置=======================
flume拉取数据的时候会先产生tmp文件到达某个条件下才会变成正式文件所有需要指定不拉取tmp文件
flume打到hdfs上时，按照文件大小生成文件，在达到指定大小之前数据都是以.tmp文件形式保存在hdfs上，hive外部表也会加载这些文件，但是当文件完成后.tmp会消失，这时候hive会报找不到文件的错误。解决方法是自己写hive的pathfilter类，hive加载数据的时候把tmp文件过滤掉不加载即可。

 错误信息如下：

自定义PathFilter类如下：
/**
 * 
   * @Title: FileFilterExcludeTmpFiles.java 
   * @Description: hive加载分区表时会加载.tmp的文件，该类型文件在flume滚动数据之后就会消失，此时hive找不到该文件就会报错
   *                     该类会将.tmp的文件过滤掉，不加载进hive的分区表中 
   * @version V0.1.0
   * @see
 */ public class FileFilterExcludeTmpFiles implements PathFilter{
    private static final Logger logger = LoggerFactory.getLogger(FileFilterExcludeTmpFiles.class);
    public boolean accept(Path path) {
        // TODO Auto-generated method stub         return !name.startsWith("_") && !name.startsWith(".") && !name.endsWith(".tmp");
    }
  
}

编写完后，打成jar包上传服务器，再修改hive-site.xml文件，修改如下：
<property>
  
    <name>hive.aux.jars.path</name><value>file:///usr/lib/mylib/FilterTmpPath.jar</value>
  
    <description>The location of the plugin jars that contain implementations of user defined functions and serdes.</description>
  
  </property>
  
  <property>
  
    <name>mapred.input.pathFilter.class</name>
  
    <value>cn.utils.hive.FileFilterExcludeTmpFiles</value>
  
  </property>

======================================================


第五部分
将hive执行结果保存到hive表中并且将HIVE表中的数据同步到mysql(使用sqoop)
其实可以这个如下的SQL(将查到的数据插入新表中)
insert into upflow select ip,sum(upflow) as sum from accesslog group by ip order by sum desc;

然后使用sqoop将hive数据导入mysql
下载sqoop http://mirrors.hust.edu.cn/apache/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz
这是2 请安装sqoop1.4

[root@vm01 bin]# sqoop version
看下是否成功


创建mysql库
create sqoop if not exists sqoop;
授权
grant all privileges on sqoop.* to 'sqoop'@'%' identified by 'sqoop'; sqoop 来自所有机器的hive metadata表的权限
grant all privileges on sqoop.* to 'sqoop'@'localhost' identified by 'sqoop'; sqoop 来自localhost机器的hive metadata表的权限
grant all privileges on sqoop.* to 'sqoop'@'vm01' identified by 'sqoop';

flush privileges;
//id int, name string, age string, tel string
CREATE TABLE Logs
(
id int,
name varchar(255),
age varchar(255),
tel varchar(255)
)

复制驱动到sqoop的lib中

导入数据到mysql中
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table Logs --direct --export-dir /weblogs/flume/2018-04-01 --driver com.mysql.jdbc.Driver --input-fields-terminated-by ','
在这里需要是直接文件夹入/weblogs/flume/2018-04-01 如果放为/weblogs/flume/好像不行
一直的疑问那如果按照日期来区分的话如何全部导入到数据库中呢？

现在按照简单的思路来做个吧
建立hive表就当访问次数来计算
再建立个hive表记录每个人的访问次数
然后将这个数据更新到mysql
前台页面将这个数据读取到页面

select name,count(name) from testweblog GROUP BY name;
结果
beijun	1
chenfangwei	1
chenwei	1
chenyong	463
fangfa	1
xiaoxun	1

注意可以调整执行器的个数
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>


创建hive表
CREATE TABLE count_sum(name string, counts INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","

将hive计算的数据更新到新表
insert into table count_sum select name,count(name) from testweblog GROUP BY name; 这样发现每次都往新表中增加
就会出现
chenyong	463
fangfa	1
chenyong	463
fangfa	1
其实之前的数据不需要就可以用
insert overwrite table count_sum select name,count(name) from testweblog GROUP BY name;

创建表结构通过sqoop将hive表推送到mysql
CREATE TABLE accessLogs
(
name varchar(255),
count int
)

插入
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --direct --export-dir /hive-data/warehouse/tony.db/count_sum --driver com.mysql.jdbc.Driver --input-fields-terminated-by ','

更新
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --export-dir /hive-data/warehouse/tony.db/count_sum --driver com.mysql.jdbc.Driver --input-fields-terminated-by ',' --update-key name --update-mode allowinsert
这条语句有问题去掉 --direct 去掉 --dricer
可以执行的语句如下
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --export-dir /hive-data/warehouse/tony.db/count_sum  --input-fields-terminated-by ',' --update-key name --update-mode allowinsert

第六部分

接下来通过azkaban将所有联系起来
下载安装gradle
解压加环境变量
设置阿里源在build.gradle里换源
repositories {
        maven {
           url 'http://maven.aliyun.com/nexus/content/groups/public/'
        }
    }

allprojects{
    repositories {
        def REPOSITORY_URL = 'http://maven.aliyun.com/nexus/content/groups/public/'
        all { ArtifactRepository repo ->
            if(repo instanceof MavenArtifactRepository){
                def url = repo.url.toString()
                if (url.startsWith('https://repo1.maven.org/maven2') || url.startsWith('https://jcenter.bintray.com/')) {
                    project.logger.lifecycle "Repository ${repo.url} replaced by $REPOSITORY_URL."
                    remove repo
                }
            }
        }
        maven {
            url REPOSITORY_URL
        }
    }
}

安装依赖
yum -y install gcc  
yum -y install gcc-c++  
yum install make  
  
-- 或者  
yum groupinstall "Development Tools"  
-- 或者  
yum install gcc gcc-c++ kernel-devel  

yum install git

下载azkaban
# Build Azkaban
./gradlew build

# Clean the build
./gradlew clean

# Build and install distributions
./gradlew installDist

# Run tests
./gradlew test

# Build without running tests
./gradlew build -x test
编译失败 又是存储不够又是什么的
下载别人编译好的自己百度云里有2.5的
三个包一个web包一个执行器包一个sql包

创建数据库和表
>> create database if not exists azkaban;
>> use azkaban;
>> source /appl/azkaban-2.5.0/create-all-sql-2.5.0.sql 


授权
grant all privileges on azkaban.* to 'azkaban'@'%' identified by 'azkaban'; 
grant all privileges on azkaban.* to 'azkaban'@'localhost' identified by 'azkaban'; 
grant all privileges on azkaban.* to 'azkaban'@'vm01' identified by 'azkaban';

创建SSL
到需要创建keystore的地方生成
cd /appl/azkaban-web/
keytool -keystore keystore -alias jetty -genkey -keyalg RSA
密码123456
Enter keystore password:  
Re-enter new password: 
What is your first and last name?
  [Unknown]:  
What is the name of your organizational unit?
  [Unknown]:  
What is the name of your organization?
  [Unknown]:  
What is the name of your City or Locality?
  [Unknown]:  
What is the name of your State or Province?
  [Unknown]:  
What is the two-letter country code for this unit?
  [Unknown]:  CN
Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN correct?
  [no]:  y

Enter key password for <jetty>
	(RETURN if same as keystore password):  


web配置文件
Azkaban Personalization Settings
azkaban.name=Test
azkaban.label=My Local Azkaban
azkaban.color=#FF3601
azkaban.default.servlet.path=/index
web.resource.dir=web/
default.timezone.id=Asia/Shanghai

#Azkaban UserManager class
user.manager.class=azkaban.user.XmlUserManager
user.manager.xml.file=conf/azkaban-users.xml

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100

# Velocity dev mode
velocity.dev.mode=false

# Azkaban Jetty server properties.
jetty.maxThreads=25
jetty.ssl.port=8443
jetty.port=8081
jetty.keystore=keystore
jetty.password=123456
jetty.keypassword=123456
jetty.truststore=keystore
jetty.trustpassword=123456
# Azkaban Executor settings
executor.port=12321

# mail settings
mail.sender=
mail.host=
job.failure.email=
job.success.email=

lockdown.create.projects=false

cache.directory=cache

添加一个用户
在conf/azkaban-users.xml 添加一个用户
<user username="admin" password="admin" roles="admin,metrics"/>

修改执行器
#Azkaban
default.timezone.id=Asia/Shanghai

# Azkaban JobTypes Plugins
azkaban.jobtype.plugin.dir=plugins/jobtypes

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100

# Azkaban Executor settings
executor.maxThreads=50
executor.port=12321
executor.flow.threads=30

启动web
启动$AK_HOME/bin/azkaban-web-start.sh 
必须在web目录跟目录执行 否则报错
bin/azkaban-web-start.sh
后天运行使用
nohup  bin/azkaban-web-start.sh  1>/tmp/azstd.out  2>/tmp/azerr.out &

浏览器访问https://192.168.8.128:8443/

启动executor
bin/azkaban-executor-start.sh
后台运行
nohup  bin/azkaban-executor-start.sh 
azkaban环境安装完成

使用azkaban
创建一个工程
一个工程中包含多个flow,一个flow包含多个job
例子创建hello.job
type=command
command=echo "data 2 hive"

但是一个flow是有多个job组成的,多个job还会有依赖关系,依赖关系通过dependencies定义
如我们这个项目中包含有,
1.HDFS2Hive
type=command
command=echo "HDFS2Hive"

2.Hive2SumHive
type=command
command=echo "Hive2SumHive"
dependencies=HDFS2Hive

3.SumHive2Mysql
type=command
command=echo "SumHive2Mysql"
dependencies=Hive2SumHive

将三个文件打包成zip上传

其他内容
其他job配置选项
可以定义job依赖另一个flow，配置
type=flow
flow.name=fisrt_flow

可以设置每个job中子命令
type=command
command=echo "hello"
command.1=echo "world"

可以配置job失败重启次数，及间隔时间,比如，上述ftp获取日志，我可以配置重试12次，每隔5分钟一次。
type=command
command=wget "ftp://file1" -O /data/file1
retries=12
#单位毫秒
retry.backoff=300000

现阶段的脚本如下：
#!/bin/bash

###############JDK
export JAVA_HOME=/appl/jdk
export PATH=$PATH:$JAVA_HOME/bin

###############HADOOP
export HADOOP_HOME=/appl/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

###############HIVE
export HIVE_HOME=/appl/hive
export PATH=$PATH:$HIVE_HOME/bin

###############FLUME
export FLUME_HOME=/appl/flume
export PATH=$PATH:$FLUME_HOME/bin

###############sqoop
export SQOOP_HOME=/appl/sqoop
export PATH=$PATH:$SQOOP_HOME/bin


HQL="set hive.mapred.supports.subdirectories=true;set mapreduce.input.fileinputformat.input.dir.recursive=true;insert overwrite table tony.count_sum select name,count(name) from tony.testweblog GROUP BY name;"
hive -e "$HQL"


第七部分
配置hadoop,azkaban-web,azkaban-executor的自动启动(还未完成)

#Start Hadoop
start-all.sh

#Start azkaban
bin/azkaban-web-start.sh
bin/azkaban-executor-start.sh


第八部分
全部联系起来
实验中发现HDFS被锁定
发现丢失blocks
使用
hdfs fsck /检查
使用
hdfs fsck / -delete修复

首先开启hadoop
然后开启flume
然后开启模拟数据
然后开始azkaban的定时任务
最后在页面上画图出来数据的图形

在这个过程中经常azkaban会莫名其妙退出,看了下内存原理执行任务的时候内存耗尽了进程被强制终止了.
调节azkaban机器内存从1G到3G后没有在发生该类现象.

生成图标的话可以用代码或者grafana,grafana比较简单

下载wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.6.2-1.x86_64.rpm
yum localinstall grafana-4.6.2-1.x86_64.rpm
或者编译好的
wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.6.0.linux-x64.tar.gz 
tar -zxvf grafana-4.2.0.linux-x64.tar.gz 

#/bin/systemctl daemon-reload //重新加载systemd 发现新的项目
#/bin/systemctl enable grafana-server.service //将服务加入开机启动
打开浏览器输入 本机 IP：3000 就可以访问了
用户名和密码默认都是 admin
到此全部安装完毕，添加数据源就可以使用了
搞了一小时数据库了没有时间维度算了直接用e-chart

例子
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>ECharts</title>
    <!-- 引入 echarts.js -->
    <script type="text/javascript" src="http://echarts.baidu.com/gallery/vendors/echarts/echarts-all-3.js"></script>
</head>
<body>
    <!-- 为ECharts准备一个具备大小（宽高）的Dom -->
    <div id="main" style="width: 600px;height:400px;"></div>
    <script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('main'));

        // 指定图表的配置项和数据
        var option = {
            title: {
                text: 'ECharts 入门示例'
            },
            tooltip: {},
            legend: {
                data:['销量']
            },
            xAxis: {
                data: ["衬衫","羊毛衫","雪纺衫","裤子","高跟鞋","袜子"]
            },
            yAxis: {},
            series: [{
                name: '销量',
                type: 'bar',
                data: [5, 20, 36, 10, 10, 20]
            }]
        };

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
    </script>
</body>
</html>

感觉还要建立个springboot项目以后再弄吧.
