SparkHadoop项目
1.安装虚拟机配置网络
修改
BOOTPROTO=static  #设置静态Ip
ONBOOT=yes  #这里如果为no的话就改为yes，表示网卡设备自动启动
增加
GATEWAY=192.168.10.2  #这里的网关地址就是第二步获取到的那个网关地址
IPADDR=192.168.10.150  #配置ip，在第二步已经设置ip处于192.168.10.xxx这个范围，我就随便设为150了，只要不和网关相同均可
NETMASK=255.255.255.0#子网掩码
DNS1=8.8.8.8#dns服务器1，填写你所在的网络可用的dns服务器地址即可

重启生效
service network restart
（这部分还没搞明白先忽略使用动态地址）

2.安装三台虚拟机
VM01 192.168.8.128
VM02 192.168.8.129
VM03 192.168.8.130

修改域名


3.安装hadoop

卸载OpenJDK 
rpm -qa | grep jdk | xargs rpm -e --nodeps  
安装JDK
###############JDK
export JAVA_HOME=/appl/jdk
export PATH=$PATH:$JAVA_HOME/bin

下载hadoop-2.6.5.tar.gz
解压
配置/etc/profile增加

export HADOOP_HOME=/appl/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

测试是否成功
hadoop version
hadoop fs -ls /

4.复制JDK hadoop profile
scp -r /appl/jdk/  root@192.168.8.130:/appl/
ln -s hadoop-2.7.5/ hadoop
scp /etc/profile  root@192.168.8.130:/etc/profile

5.配置主机名对于ip  配置SSH 免密码
/etc/hosts 增加对于关系
修改主机名
/etc/hostname vm01
scp /etc/hosts root@vm03:/etc/hosts

生成秘钥 三个机器都生成下
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
拷贝秘钥
scp ~/.ssh/id_rsa.pub root@vm03:~/.ssh/authorized_keys

6.修改hadoop配置
core-site.xml
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://vm01/</value>
	    </property>
	</configuration>

hdfs-site.xml
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	   <name>dfs.replication</name>
	   <value>2</value>
	</property>
	</configuration>

	cp mapred-site.xml.template mapred-site.xml
	<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	       <name>mapreduce.framework.name</name>
	       <value>yarn</value>
	</property>
	</configuration>

nano yarn-site.xml
	<?xml version="1.0"?>
	<configuration>
	   <property>
	        <name>yarn.resourcemanager.hostname</name>
	        <value>vm01</value>
	   </property>
	   <property>
	        <name>yarn.nodemanager.aux-services</name>
	        <value>mapreduce_shuffle</value>
	   </property>
	</configuration>

slaves
	vm02
	vm03

nano hadoop-env.sh 
export JAVA_HOME=/appl/jdk


7. 分发hadoop配置
scp -r  hadoop/ root@vm03:/appl/hadoop/etc/

8.格式化文件
hadoop namenode -format

9.启动hadoop
start-all.sh

10.启动成功
[root@vm01 etc]# jps
1369 NameNode
1549 SecondaryNameNode
1694 ResourceManager
1951 Jps
启动成功

hadoopadmin 界面 http://192.168.8.128:50070/

第二部分
利用hadoop自带的demo来计算wordcount
mkdir input
   87  cd input/
   88  touch test01.txt
   89  echo 'hello world' >> test01.txt 
   90  echo 'hello hadoop' >> test02.txt 
   91  echo 'hello mapreduce' >> test02.txt
   93  hadoop fs -mkdir /wc_input
   94  hadoop fs -lsr /
   95  hadoop fs -put test* /wc_input


cd /appl/hadoop/share/hadoop/mapreduce/
运行jar包
hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wc_input /wc_output
运行完成
查看结果
hadoop fs cat /wc_output/part-r-00000

附加作业：写一个小教本如jps打印出三台机器的状态


附加：
防止时间不同步每台机器安装ntp
yum install ntp

然后通过ntpdate asia.pool.ntp.org

#crontab -e 
*/10 * * * *  /usr/sbin/ntpdate -u asia.pool.ntp.org >/dev/null 2>&1
#service crond restart


第三部分
Hive的工作原理：将文本数据与表结构数据做mapping,这样就可以通过SQL来查询.
将关联关系存在mysql中.
安装依赖库
搜索yum search libaio
yun install libaio

检查mysql是否安装
yum list installed |grep mysql

如果有卸载
yum remove -y xxx

下载仓库
wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm (centos7)
导入本地仓库
yum localinstall mysql-community-release-el7-5.noarch.rpm
验证本地仓库是否导入
yum repolist enabled | grep "mysql.*-community.*"
查看所有版本
yum repolist all | grep mysql
启用禁用某些版本
yum-config-manager --disable mysql56-community
yum-config-manager --enable mysql57-community-dmr

安装
yum install mysql-community-server 

启动mysql
systemctl start mysql (centos修改为了systemctl)
systemctl status mysql

进入mysql
mysql
>> show databases;
>> create database if not exists hive_metadata;

授权
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive'; 为hive：hive 来自所有机器的hive metadata表的权限
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive'; 为hive：hive 来自localhost机器的hive metadata表的权限
生效权限
flush privileges;

准备好环境后下载hive包随便选取一个版本2.1.2
解压后设置环境变量
###############HIVE
export HIVE_HOME=/appl/hive
export PATH=$PATH:$HIVE_HOME/bin
source /etc/profile

由于要连接mysql需要连接驱动下载mysql-connector-java-5.1.31.jar
放入hive目录下的lib文件夹中 拷贝完成设置配置文件

cp hive-env.sh.template hive-env.sh
cp hive-default.xml.template hive-site.xml
cp hive-log4j2.properties.template hive-log4j2.properties
cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties


解放hive-env.sh
mv hive-env.sh.template hive-env.sh
修改为
export JAVA_HOME=/appl/jdk
# Set HADOOP_HOME to point to a specific hadoop install directory
export HADOOP_HOME=/appl/hadoop       
# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/appl/hive/conf
# Folder containing extra ibraries required for hive compilation/execution can be controlled by:
export HIVE_AUX_JARS_PATH=/appl/hive/lib 

修改配置文件名为hive-site.xml 修改连接
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://192.168.8.128:3306/hive_metadata?createDatabaseIfNotExist=true</value>

<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>

<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>

<name>javax.jdo.option.ConnectionPassword</name>
<value>hive</value>

配置hive数据存储位置
新增三个存储文件夹由于hive依赖hdfs 所以要在hdfs建立存储文件夹
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/warehouse
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/tmp
[root@vm01 conf]# hadoop fs -mkdir -p /hive-data/log
hadoop fs -chmod -R 777 /hive-data/warehouse
hadoop fs -chmod -R 777 /hive-data/tmp
hadoop fs -chmod -R 777 /hive-data/log


<property>
    <name>hive.exec.scratchdir</name>
    <value>/hive-data/tmp</value>
</property>
HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。

<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/hive-data/warehouse</value>
</property>
Hive 默认的数据文件存储路径，通常为 HDFS 可写的路径。 

<property>
    <name>hive.querylog.location</name>
    <value>/hive-data/log</value>
</property>
Hive 实时查询日志所在的目录，如果该值为空，将不创建实时的查询日志。 通常为 HDFS 可写的路径

设置本地临时目录
mkdir /appl/hive-tmp/
把{system:java.io.tmpdir} 改成 /appl/hive-tmp/

把 {system:user.name} 改成 {user.name}

初始化元数据到mysql
./schematool -initSchema -dbType mysql
初始化完成


hive 进入命令行
create database tony;

use tont;

create table test (mykey string,myval string);

insert into test values("1","www.ymq.io");

select * from test; 查处结果表示成功

创建逗号分隔的表
create table t2(id int, name string, age string, tel string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

创建a.txt
1,chenyong,32
2,fangfa,28
3.chenwei,31
4.chenfangwei,32
5.beijun,28
6.xiaoxun,28

将本地数据和hive表结构关联

LOAD DATA LOCAL INPATH '/appl/a.txt' OVERWRITE INTO TABLE t2; 本地数据导入
LOAD DATA INPATH '/appl/a.txt' OVERWRITE INTO TABLE t2; HDFS数据导入


第四部分 Flume

Flume概念：
Flume可以直接到HDFS或者到kafka里面
源source(文件.文件夹),管道channel,目标sink(hdfs.kafka)

下载flume1.8
解压
做软链接
ln -s apache-flume-1.8.0-bin/ ./flume

添加环境变量
###############FLUME
export FLUME_HOME=/appl/flume
export PATH=$PATH:$FLUME_HOME/bin

source /etc/profile

flume-ng version 查看版本

修改flume的变量在conf/flume-env.sh
添加java环境变量
export JAVA_HOME=/appl/jdk/

创建监听配置文件在conf目录下创建一个net-logger-test.conf
写入内容
	# Name the components on this agent  
	a1.sources = r1  
	a1.sinks = k1  
	a1.channels = c1  
	  
	# Describe/configure the source  
	a1.sources.r1.type = netcat  
	a1.sources.r1.bind = localhost  
	a1.sources.r1.port = 44444  
	  
	# Describe the sink  
	a1.sinks.k1.type = logger  
	  
	# Use a channel which buffers events in memory  
	a1.channels.c1.type = memory  
	a1.channels.c1.capacity = 1000  
	a1.channels.c1.transactionCapacity = 100  
	  
	# Bind the source and sink to the channel  
	a1.sources.r1.channels = c1  
	a1.sinks.k1.channel = c1  

flume-ng agent -n a1 -c conf -f /appl/flume/conf/net-logger-test.conf -Dflume.root.logger=INFO,console
-n 表示名字
-c 制定配置文件所在目录
-f 制定采集方案文件

测试使用nc
nc localhost 44444
>> hello

实例源监听文件夹=>目标输出到HDFS
	#agent名， source、channel、sink的名称
	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1
	#具体定义source
	a1.sources.r1.type = spooldir
	a1.sources.r1.spoolDir = /appl/monitor
	#具体定义channel
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 10000
	a1.channels.c1.transactionCapacity = 100
	#具体定义sink
	a1.sinks.k1.type = hdfs
	a1.sinks.k1.hdfs.path = hdfs://vm01/weblogs/flume/%Y-%m-%d
	a1.sinks.k1.hdfs.filePrefix = access_log
	a1.sinks.k1.hdfs.fileType = DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp = true
	#不按照条数生成文件
	a1.sinks.k1.hdfs.rollCount = 0
	#HDFS上的文件达到128M时生成一个文件
	a1.sinks.k1.hdfs.rollSize = 134217728
	#HDFS上的文件达到60秒生成一个文件
	a1.sinks.k1.hdfs.rollInterval = 60
	 
	#组装source、channel、sink
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1


flume支持的source种类
常用的有
net
spooldir 监听目录将目录文件读取读完后设置为OK 不实时 支持断点
对指定目录进行实时监控，如发现目录新增文件，立刻收集并发送
缺点：不能对目录文件进行修改，如果有追加内容的文本文件，不允许，

exec 执行如tail -f 监听单个文件 比较实时但是不支持断点
taildirSource 实时支持文件夹中的匹配的所有文件的监控支持断点--->这个是最好的最实用的


taildir实例
	#agent名， source、channel、sink的名称
	a1.sources = r1
	a1.channels = c1
	a1.sinks = k1
	#具体定义source
	a1.sources.r1.type = TAILDIR
	a1.sources.r1.channels = c1
	a1.sources.r1.positionFile = /appl/monitor/taildir_position.json  
	a1.sources.r1.filegroups = f1                          
	a1.sources.r1.filegroups.f1 = /appl/monitor/.*.log  
	a1.sources.r1.headers.f1.headerKey1 = value1             
	a1.sources.r1.fileHeader = true

	#具体定义channel
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 10000
	a1.channels.c1.transactionCapacity = 100
	#具体定义sink
	a1.sinks.k1.type = hdfs
	a1.sinks.k1.hdfs.path = hdfs://vm01/weblogs/flume/%Y-%m-%d
	a1.sinks.k1.hdfs.filePrefix = access_log
	a1.sinks.k1.hdfs.fileType = DataStream
	a1.sinks.k1.hdfs.useLocalTimeStamp = true
	#不按照条数生成文件
	a1.sinks.k1.hdfs.rollCount = 0
	#HDFS上的文件达到128M时生成一个文件
	a1.sinks.k1.hdfs.rollSize = 1024
	#HDFS上的文件达到60秒生成一个文件
	a1.sinks.k1.hdfs.rollInterval = 0
	 
	#组装source、channel、sink
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1

##########################################Hive外表说明######################
在hive中，外表是个很重要的组成部分，通过外表可以很方便进行数据的共享。
因为普通的表会将数据文件拷贝自己的目录下，这样想要分享数据只能保存多份数据。
但是外表很好的解决了这个问题。
CREATE EXTERNAL TABLE sunwg_test09(id INT, name string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ‘\t’
LOCATION ‘/sunwg/test08′;

上面的语句创建了一张名字为sunwg_test09的外表，该表有id和name两个字段，
字段的分割符为tab，文件的数据文件夹为/sunwg/test08

实际上外表不光可以指定hdfs的目录，本地的目录也是可以的。
比如：
CREATE EXTERNAL TABLE test10(id INT, name string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ‘\t’
LOCATION ‘file:////home/hjl/sunwg/’;

#######################################MapReduce和Hive支持文件夹递归##########
一般情况下，传递给MapReduce和Hive的input文件夹中不能包含子目录，否则就会报错。但后来增加了递归遍历Input目录的功能，
这个貌似是从0.23开始的，具体不清楚，反正在0.20中是不支持的。
MapReduce
默认情况下，mapreduce.input.fileinputformat.input.dir.recursive为flase.
设置mapreduce.input.fileinputformat.input.dir.recursive=true，这个参数是客户端参数，可以在MapReduce中设置，也可以在mapred-site.xml中设置，无所谓。

CREATE EXTERNAL TABLE lxw1234 (d string) stored AS textfile location '/tmp/lxw1234/';
查询：select * from lxw1234;
同样报错 “Not a file: hdfs://cdh5/tmp/lxw1234/subdir” 。

在hive-cli中设置参数：
set hive.mapred.supports.subdirectories=true;
set mapreduce.input.fileinputformat.input.dir.recursive=true;
也可以Hive脚本中添加

实例：

CREATE EXTERNAL TABLE tony.testweblog(id int, name string, age string, tel string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/weblogs/flume/';

这样就讲flume收集上来的数据和hive挂钩了
下面需要写个代码定时产生相关数据

实验脚本：
内容：定时差生如下格式数据
id int, name string, age string, tel string
1,chenyong,32,18721688888

脚本内容
#!/bin/bash  

MAX=10000

function rand(){    
    min=$1    
    max=$(($2-$min+1))    
    num=$(($RANDOM+1000000000)) #增加一个10位的数再求余    
    echo $(($num%$max+$min))    
}

for (( i = 0; i < MAX ; i ++ ))
do
	RAND_NUM=$(rand 1 6)
	if [ "$RAND_NUM" -eq "1" ];then
    	echo "$i,beijun,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "2" ];then
    	echo "$i,chenfangwei,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "3" ];then
    	echo "$i,chenwei,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "4" ];then
    	echo "$i,chenyong,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "5" ];then
    	echo "$i,fangfa,32,123" >> /appl/monitor/1.log
    fi
    if [ "$RAND_NUM" -eq "6" ];then
    	echo "$i,xiaoxun,32,123" >> /appl/monitor/1.log
    fi
    sleep 1s
done

==================hive的特殊设置=======================
flume拉取数据的时候会先产生tmp文件到达某个条件下才会变成正式文件所有需要指定不拉取tmp文件
flume打到hdfs上时，按照文件大小生成文件，在达到指定大小之前数据都是以.tmp文件形式保存在hdfs上，hive外部表也会加载这些文件，但是当文件完成后.tmp会消失，这时候hive会报找不到文件的错误。解决方法是自己写hive的pathfilter类，hive加载数据的时候把tmp文件过滤掉不加载即可。

 错误信息如下：

自定义PathFilter类如下：
/**
 * 
   * @Title: FileFilterExcludeTmpFiles.java 
   * @Description: hive加载分区表时会加载.tmp的文件，该类型文件在flume滚动数据之后就会消失，此时hive找不到该文件就会报错
   *                     该类会将.tmp的文件过滤掉，不加载进hive的分区表中 
   * @version V0.1.0
   * @see
 */ public class FileFilterExcludeTmpFiles implements PathFilter{
    private static final Logger logger = LoggerFactory.getLogger(FileFilterExcludeTmpFiles.class);
    public boolean accept(Path path) {
        // TODO Auto-generated method stub         return !name.startsWith("_") && !name.startsWith(".") && !name.endsWith(".tmp");
    }
  
}

编写完后，打成jar包上传服务器，再修改hive-site.xml文件，修改如下：
<property>
  
    <name>hive.aux.jars.path</name><value>file:///usr/lib/mylib/FilterTmpPath.jar</value>
  
    <description>The location of the plugin jars that contain implementations of user defined functions and serdes.</description>
  
  </property>
  
  <property>
  
    <name>mapred.input.pathFilter.class</name>
  
    <value>cn.utils.hive.FileFilterExcludeTmpFiles</value>
  
  </property>

======================================================


第五部分
将hive执行结果保存到hive表中并且将HIVE表中的数据同步到mysql(使用sqoop)
其实可以这个如下的SQL(将查到的数据插入新表中)
insert into upflow select ip,sum(upflow) as sum from accesslog group by ip order by sum desc;

然后使用sqoop将hive数据导入mysql
下载sqoop http://mirrors.hust.edu.cn/apache/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz
这是2 请安装sqoop1.4

[root@vm01 bin]# sqoop version
看下是否成功


创建mysql库
create sqoop if not exists sqoop;
授权
grant all privileges on sqoop.* to 'sqoop'@'%' identified by 'sqoop'; sqoop 来自所有机器的hive metadata表的权限
grant all privileges on sqoop.* to 'sqoop'@'localhost' identified by 'sqoop'; sqoop 来自localhost机器的hive metadata表的权限
grant all privileges on sqoop.* to 'sqoop'@'vm01' identified by 'sqoop';

flush privileges;
//id int, name string, age string, tel string
CREATE TABLE Logs
(
id int,
name varchar(255),
age varchar(255),
tel varchar(255)
)

复制驱动到sqoop的lib中

导入数据到mysql中
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table Logs --direct --export-dir /weblogs/flume/2018-04-01 --driver com.mysql.jdbc.Driver --input-fields-terminated-by ','
在这里需要是直接文件夹入/weblogs/flume/2018-04-01 如果放为/weblogs/flume/好像不行
一直的疑问那如果按照日期来区分的话如何全部导入到数据库中呢？

现在按照简单的思路来做个吧
建立hive表就当访问次数来计算
再建立个hive表记录每个人的访问次数
然后将这个数据更新到mysql
前台页面将这个数据读取到页面

select name,count(name) from testweblog GROUP BY name;
结果
beijun	1
chenfangwei	1
chenwei	1
chenyong	463
fangfa	1
xiaoxun	1

注意可以调整执行器的个数
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>


创建hive表
CREATE TABLE count_sum(name string, counts INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","

将hive计算的数据更新到新表
insert into table count_sum select name,count(name) from testweblog GROUP BY name; 这样发现每次都往新表中增加
就会出现
chenyong	463
fangfa	1
chenyong	463
fangfa	1
其实之前的数据不需要就可以用
insert overwrite table count_sum select name,count(name) from testweblog GROUP BY name;

创建表结构通过sqoop将hive表推送到mysql
CREATE TABLE accessLogs
(
name varchar(255),
count int
)

插入
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --direct --export-dir /hive-data/warehouse/tony.db/count_sum --driver com.mysql.jdbc.Driver --input-fields-terminated-by ','

更新
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --export-dir /hive-data/warehouse/tony.db/count_sum --driver com.mysql.jdbc.Driver --input-fields-terminated-by ',' --update-key name --update-mode allowinsert
这条语句有问题去掉 --direct 去掉 --dricer
可以执行的语句如下
sqoop export --connect jdbc:mysql://vm01:3306/sqoop --username sqoop --password sqoop --table accessLogs --export-dir /hive-data/warehouse/tony.db/count_sum  --input-fields-terminated-by ',' --update-key name --update-mode allowinsert

第六部分

接下来通过azkaban将所有联系起来
下载安装gradle
解压加环境变量
设置阿里源在build.gradle里换源
repositories {
        maven {
           url 'http://maven.aliyun.com/nexus/content/groups/public/'
        }
    }

allprojects{
    repositories {
        def REPOSITORY_URL = 'http://maven.aliyun.com/nexus/content/groups/public/'
        all { ArtifactRepository repo ->
            if(repo instanceof MavenArtifactRepository){
                def url = repo.url.toString()
                if (url.startsWith('https://repo1.maven.org/maven2') || url.startsWith('https://jcenter.bintray.com/')) {
                    project.logger.lifecycle "Repository ${repo.url} replaced by $REPOSITORY_URL."
                    remove repo
                }
            }
        }
        maven {
            url REPOSITORY_URL
        }
    }
}

安装依赖
yum -y install gcc  
yum -y install gcc-c++  
yum install make  
  
-- 或者  
yum groupinstall "Development Tools"  
-- 或者  
yum install gcc gcc-c++ kernel-devel  

yum install git

下载azkaban
# Build Azkaban
./gradlew build

# Clean the build
./gradlew clean

# Build and install distributions
./gradlew installDist

# Run tests
./gradlew test

# Build without running tests
./gradlew build -x test
编译失败 又是存储不够又是什么的
下载别人编译好的自己百度云里有2.5的
三个包一个web包一个执行器包一个sql包

创建数据库和表
>> create database if not exists azkaban;
>> use azkaban;
>> source /appl/azkaban-2.5.0/create-all-sql-2.5.0.sql 


授权
grant all privileges on azkaban.* to 'azkaban'@'%' identified by 'azkaban'; 
grant all privileges on azkaban.* to 'azkaban'@'localhost' identified by 'azkaban'; 
grant all privileges on azkaban.* to 'azkaban'@'vm01' identified by 'azkaban';

创建SSL
到需要创建keystore的地方生成
cd /appl/azkaban-web/
keytool -keystore keystore -alias jetty -genkey -keyalg RSA
密码123456
Enter keystore password:  
Re-enter new password: 
What is your first and last name?
  [Unknown]:  
What is the name of your organizational unit?
  [Unknown]:  
What is the name of your organization?
  [Unknown]:  
What is the name of your City or Locality?
  [Unknown]:  
What is the name of your State or Province?
  [Unknown]:  
What is the two-letter country code for this unit?
  [Unknown]:  CN
Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN correct?
  [no]:  y

Enter key password for <jetty>
	(RETURN if same as keystore password):  


web配置文件
Azkaban Personalization Settings
azkaban.name=Test
azkaban.label=My Local Azkaban
azkaban.color=#FF3601
azkaban.default.servlet.path=/index
web.resource.dir=web/
default.timezone.id=Asia/Shanghai

#Azkaban UserManager class
user.manager.class=azkaban.user.XmlUserManager
user.manager.xml.file=conf/azkaban-users.xml

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100

# Velocity dev mode
velocity.dev.mode=false

# Azkaban Jetty server properties.
jetty.maxThreads=25
jetty.ssl.port=8443
jetty.port=8081
jetty.keystore=keystore
jetty.password=123456
jetty.keypassword=123456
jetty.truststore=keystore
jetty.trustpassword=123456
# Azkaban Executor settings
executor.port=12321

# mail settings
mail.sender=
mail.host=
job.failure.email=
job.success.email=

lockdown.create.projects=false

cache.directory=cache

添加一个用户
在conf/azkaban-users.xml 添加一个用户
<user username="admin" password="admin" roles="admin,metrics"/>

修改执行器
#Azkaban
default.timezone.id=Asia/Shanghai

# Azkaban JobTypes Plugins
azkaban.jobtype.plugin.dir=plugins/jobtypes

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

database.type=mysql
mysql.port=3306
mysql.host=localhost
mysql.database=azkaban
mysql.user=azkaban
mysql.password=azkaban
mysql.numconnections=100

# Azkaban Executor settings
executor.maxThreads=50
executor.port=12321
executor.flow.threads=30

启动web
启动$AK_HOME/bin/azkaban-web-start.sh 
必须在web目录跟目录执行 否则报错
bin/azkaban-web-start.sh
后天运行使用
nohup  bin/azkaban-web-start.sh  1>/tmp/azstd.out  2>/tmp/azerr.out &

浏览器访问https://192.168.8.128:8443/

启动executor
bin/azkaban-executor-start.sh
后台运行
nohup  bin/azkaban-executor-start.sh 
azkaban环境安装完成

使用azkaban
创建一个工程
一个工程中包含多个flow,一个flow包含多个job
例子创建hello.job
type=command
command=echo "data 2 hive"

但是一个flow是有多个job组成的,多个job还会有依赖关系,依赖关系通过dependencies定义
如我们这个项目中包含有,
1.HDFS2Hive
type=command
command=echo "HDFS2Hive"

2.Hive2SumHive
type=command
command=echo "Hive2SumHive"
dependencies=HDFS2Hive

3.SumHive2Mysql
type=command
command=echo "SumHive2Mysql"
dependencies=Hive2SumHive

将三个文件打包成zip上传

其他内容
其他job配置选项
可以定义job依赖另一个flow，配置
type=flow
flow.name=fisrt_flow

可以设置每个job中子命令
type=command
command=echo "hello"
command.1=echo "world"

可以配置job失败重启次数，及间隔时间,比如，上述ftp获取日志，我可以配置重试12次，每隔5分钟一次。
type=command
command=wget "ftp://file1" -O /data/file1
retries=12
#单位毫秒
retry.backoff=300000

现阶段的脚本如下：
#!/bin/bash

###############JDK
export JAVA_HOME=/appl/jdk
export PATH=$PATH:$JAVA_HOME/bin

###############HADOOP
export HADOOP_HOME=/appl/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

###############HIVE
export HIVE_HOME=/appl/hive
export PATH=$PATH:$HIVE_HOME/bin

###############FLUME
export FLUME_HOME=/appl/flume
export PATH=$PATH:$FLUME_HOME/bin

###############sqoop
export SQOOP_HOME=/appl/sqoop
export PATH=$PATH:$SQOOP_HOME/bin


HQL="set hive.mapred.supports.subdirectories=true;set mapreduce.input.fileinputformat.input.dir.recursive=true;insert overwrite table tony.count_sum select name,count(name) from tony.testweblog GROUP BY name;"
hive -e "$HQL"


第七部分
配置hadoop,azkaban-web,azkaban-executor的自动启动(还未完成)

#Start Hadoop
start-all.sh

#Start azkaban
bin/azkaban-web-start.sh
bin/azkaban-executor-start.sh


第八部分
全部联系起来
实验中发现HDFS被锁定
发现丢失blocks
使用
hdfs fsck /检查
使用
hdfs fsck / -delete修复

首先开启hadoop
然后开启flume
然后开启模拟数据
然后开始azkaban的定时任务
最后在页面上画图出来数据的图形

在这个过程中经常azkaban会莫名其妙退出,看了下内存原理执行任务的时候内存耗尽了进程被强制终止了.
调节azkaban机器内存从1G到3G后没有在发生该类现象.

生成图标的话可以用代码或者grafana,grafana比较简单

下载wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.6.2-1.x86_64.rpm
yum localinstall grafana-4.6.2-1.x86_64.rpm
或者编译好的
wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-4.6.0.linux-x64.tar.gz 
tar -zxvf grafana-4.2.0.linux-x64.tar.gz 

#/bin/systemctl daemon-reload //重新加载systemd 发现新的项目
#/bin/systemctl enable grafana-server.service //将服务加入开机启动
打开浏览器输入 本机 IP：3000 就可以访问了
用户名和密码默认都是 admin
到此全部安装完毕，添加数据源就可以使用了
搞了一小时数据库了没有时间维度算了直接用e-chart

例子
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>ECharts</title>
    <!-- 引入 echarts.js -->
    <script type="text/javascript" src="http://echarts.baidu.com/gallery/vendors/echarts/echarts-all-3.js"></script>
</head>
<body>
    <!-- 为ECharts准备一个具备大小（宽高）的Dom -->
    <div id="main" style="width: 600px;height:400px;"></div>
    <script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('main'));

        // 指定图表的配置项和数据
        var option = {
            title: {
                text: 'ECharts 入门示例'
            },
            tooltip: {},
            legend: {
                data:['销量']
            },
            xAxis: {
                data: ["衬衫","羊毛衫","雪纺衫","裤子","高跟鞋","袜子"]
            },
            yAxis: {},
            series: [{
                name: '销量',
                type: 'bar',
                data: [5, 20, 36, 10, 10, 20]
            }]
        };

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
    </script>
</body>
</html>

感觉还要建立个springboot项目以后再弄吧.


安装scala 下载tar包然后解压加环境变量
###############scala
export SCALA_HOME=/appl/scala
export PATH=$PATH:$SCALA_HOME/bin

使用Spark
下载Spark包
配置环境变量
###############spark
export SPARK_HOME=/appl/spark
export PATH=$PATH:$SPARK_HOME/bin
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib


修改配置文件
cp spark-env.sh.template spark-env.sh

export JAVA_HOME=/appl/jdk
export SCALA_HOME=/appl/scala
export SPARK_MASTER_IP=vm01
export SPARK_WORKING_MEMORY=1g
export HADOOP_CONF_DIR=/appl/hadoop/etc/hadoop/

修改slaves
cp slaves.template slaves
修改为
vm02
vm03

一次拷贝spark和profile到各个机器里面
scp -r /appl/scala-2.11.12/ root@vm03:/appl/

启动程序
在spark目录的sbin调用start-all.sh程序
jps看下有没有master/slave
看8080端口是否有网站在用
看下spark-shell是否可以


搭建spark IDEA 测试环境JAVA
1. 直接包含开发包
	<dependencies>
        <dependency> <!-- Spark dependency -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.0</version>
        </dependency>
    </dependencies>
2. 写个出现次数的分析代码
public class TestMain {

    public static void main(String[] args){
        // 第一步：创建SparkConf对象,设置相关配置信息
        SparkConf conf = new SparkConf();
        conf.setAppName("wordcount");
        conf.setMaster("local");

        // 第二步：创建JavaSparkContext对象，SparkContext是Spark的所有功能的入口
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 第三步：创建一个初始的RDD
        // SparkContext中，用于根据文件类型的输入源创建RDD的方法，叫做textFile()方法
        JavaRDD<String> lines = sc.textFile("C:/project/01SparkHadoop-TV/test.txt");

        // 第四步：对初始的RDD进行transformation操作，也就是一些计算操作
        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            private static final long serialVersionUID = 1L;
            public Iterator<String> call(String line) {
                return Arrays.asList(line.split(" ")).iterator();

            }
        });
        JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {
            private static final long serialVersionUID = 1L;
            public Tuple2<String, Integer> call(String word) {
                return new Tuple2<String, Integer>(word, 1);
            }
        });

        JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(new Function2<Integer, Integer, Integer>() {
            private static final long serialVersionUID = 1L;
            public Integer call(Integer v1, Integer v2) {
                return v1 + v2;
            }
        });
        wordCounts.foreach(new VoidFunction<Tuple2<String,Integer>>() {
            private static final long serialVersionUID = 1L;
            public void call(Tuple2<String, Integer> wordCount){
                System.out.println(wordCount._1 + "------" + wordCount._2+"times.");
            }
        });
        sc.close();
    }
}


3. 分析文本
11 22
adad sda
wwd
dad ewrw
22
33
43 435

运行就行了
dad------1times.
adad------1times.
wwd------1times.
sda------1times.
435------1times.
22------2times.
ewrw------1times.
33------1times.
43------1times.
11------1times.

这样就可以给予java写spark代码了.也可以通过scala但是需要配置本地的hadoop和本地的spark下回再配置

这只是简单的spark分析

还需要依赖spark-sql spark-hive

配置如下配置编译插件,打包插件,执行插件没配置,已经maven依赖的包
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.test</groupId>
    <artifactId>Test001</artifactId>
    <version>1.0-SNAPSHOT</version>
    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <jdk.version>1.8</jdk.version>
    </properties>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass>com.tony.TestMain</mainClass>
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>



        </plugins>
    </build>

    <dependencies>
        <dependency> <!-- Spark dependency -->
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.1.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.1.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.1.0</version>
            <scope>provided</scope>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka_2.11</artifactId>
            <version>1.6.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.6.0</version>
        </dependency>
    </dependencies>
</project>


打包后增加运行脚本
脚本如下：
#!/bin/bash

/appl/spark/bin/spark-submit \
--master spark://vm01:7077 \
--class com.tony.TestMain \
--num-executors 1 \
--driver-memory 470m \
--executor-memory 470m \
--executor-cores 1 \
/appl/shells/Test001-1.0-SNAPSHOT-jar-with-dependencies.jar \


集群环境也可增加主节点位置如下一行
--master spark://vm01:7077 \

如果使用hivecontext的话
--files /usr/local/app/hive/conf/hive-site.xml \

如果使用mysql的话增加
--driver-class-path /usr/local/app/hive/lib/mysql-connector-java-5.1.17.jar \

spark参数调优
摘要
　　1.num-executors
　　2.executor-memory
　　3.executor-cores
　　4.driver-memory
　　5.spark.default.parallelism
　　6.spark.storage.memoryFraction
　　7.spark.shuffle.memoryFraction
　　8.total-executor-cores
　　9.资源参数参考示例
内容

1.num-executors
参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。
参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。

2.executor-memory
参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。
参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。

3.executor-cores
参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。
参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。

4.driver-memory
参数说明：该参数用于设置Driver进程的内存。
参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。

5.spark.default.parallelism
参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。
参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。

6.spark.storage.memoryFraction
参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。
参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。

7.spark.shuffle.memoryFraction
参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。
参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。

8.total-executor-cores
参数说明：Total cores for all executors.

9.资源参数参考示例
以下是一份spark-submit命令的示例：

./bin/spark-submit \
  --master spark://192.168.1.1:7077 \
  --num-executors 100 \
  --executor-memory 6G \
  --executor-cores 4 \
　--total-executor-cores 400 \ ##standalone default all cores 
  --driver-memory 1G \
  --conf spark.default.parallelism=1000 \
  --conf spark.storage.memoryFraction=0.5 \
  --conf spark.shuffle.memoryFraction=0.3 \



  Spark的RDD介绍
  普通文件的RDD化
  public static void main(String[] args){
        // 第一步：创建SparkConf对象,设置相关配置信息
        SparkConf conf = new SparkConf();
        conf.setAppName("wordcount");
        conf.setMaster("local");

        // 第二步：创建JavaSparkContext对象，SparkContext是Spark的所有功能的入口
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<Integer> numberRDD = sc.parallelize(Arrays.asList(1,2,3,4,5,6,7,8,9,0));

        Integer sum = numberRDD.reduce(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        });

        System.out.println("结果和：" + sum);
        sc.close();
    }

    RDD Tracsform操作包括 8大金刚
    map 		1->1
    filter 		n->m
    flatmap     1->n
    mapToPair
    groupbykey  
    reducebykey
    sortbykey
    join
    cogroup

    RDD Action操作
    reduce
    collect
    count
    take n
    saveastext
    countbykey
    foreach

    map演示
    public static void map(){
        // 第一步：创建SparkConf对象,设置相关配置信息
        SparkConf conf = new SparkConf();
        conf.setAppName("wordcount");
        conf.setMaster("local");

        // 第二步：创建JavaSparkContext对象，SparkContext是Spark的所有功能的入口
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<Integer> numberRDD = sc.parallelize(Arrays.asList(1,2,3,4,5,6,7,8,9,0));
        JavaRDD<Integer> result =  numberRDD.map(new Function<Integer, Integer>() {

            @Override
            public Integer call(Integer v1) throws Exception {
                return v1 * v1;
            }
        });
        result.foreach(new VoidFunction<Integer>() {
            @Override
            public void call(Integer integer) throws Exception {
                System.out.println("结果和：" + integer);
            }
        });
        sc.close();
    }

    filter演示
    public static void filter(){
        // 第一步：创建SparkConf对象,设置相关配置信息
        SparkConf conf = new SparkConf();
        conf.setAppName("wordcount");
        conf.setMaster("local");

        // 第二步：创建JavaSparkContext对象，SparkContext是Spark的所有功能的入口
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<Integer> numberRDD = sc.parallelize(Arrays.asList(1,2,3,4,5,6,7,8,9,0));
        JavaRDD<Integer> result = numberRDD.filter(new Function<Integer, Boolean>() {
            @Override
            public Boolean call(Integer v1) throws Exception {
                return (v1%2==0);
            }
        });
        result.foreach(new VoidFunction<Integer>() {
            @Override
            public void call(Integer integer) throws Exception {
                System.out.println("结果和：" + integer);
            }
        });
    }

    flatmap演示
    public static void flatmap(){
        // 第一步：创建SparkConf对象,设置相关配置信息
        SparkConf conf = new SparkConf();
        conf.setAppName("wordcount");
        conf.setMaster("local");

        // 第二步：创建JavaSparkContext对象，SparkContext是Spark的所有功能的入口
        JavaSparkContext sc = new JavaSparkContext(conf);
        JavaRDD<String> lines = sc.textFile("C:/project/01SparkHadoop-TV/test.txt");
        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterator<String> call(String s) throws Exception {
                return Arrays.asList(s.split(" ")).iterator();
            }
        });
        words.foreach(new VoidFunction<String>() {
            @Override
            public void call(String s) throws Exception {
                System.out.println("结果和：" + s);
            }
        });
        sc.close();
    }
    


