常用    
    //0.用element创建DataStream(fromElements)
    val ds0: DataStream[String] = senv.fromElements("spark", "flink")
    ds0.print()

    //1.用Tuple创建DataStream(fromElements)
    val ds1: DataStream[(Int, String)] = senv.fromElements((1, "spark"), (2, "flink"))
    ds1.print()

    //2.用Array创建DataStream
    val ds2: DataStream[String] = senv.fromCollection(Array("spark", "flink"))
    ds2.print()

    //3.用ArrayBuffer创建DataStream
    val ds3: DataStream[String] = senv.fromCollection(ArrayBuffer("spark", "flink"))
    ds3.print()

    //4.用List创建DataStream
    val ds4: DataStream[String] = senv.fromCollection(List("spark", "flink"))
    ds4.print()

    //5.用List创建DataStream
    val ds5: DataStream[String] = senv.fromCollection(ListBuffer("spark", "flink"))
    ds5.print()

    //6.用Vector创建DataStream
    val ds6: DataStream[String] = senv.fromCollection(Vector("spark", "flink"))
    ds6.print()

    //7.用Queue创建DataStream
    val ds7: DataStream[String] = senv.fromCollection(Queue("spark", "flink"))
    ds7.print()

    //8.用Stack创建DataStream
    val ds8: DataStream[String] = senv.fromCollection(Stack("spark", "flink"))
    ds8.print()

    //9.用Stream创建DataStream（Stream相当于lazy List，避免在中间过程中生成不必要的集合）
    val ds9: DataStream[String] = senv.fromCollection(Stream("spark", "flink"))
    ds9.print()

    //10.用Seq创建DataStream
    val ds10: DataStream[String] = senv.fromCollection(Seq("spark", "flink"))
    ds10.print()

    //11.用Set创建DataStream(不支持)
    //val ds11: DataStream[String] = senv.fromCollection(Set("spark", "flink"))
    //ds11.print()

    //12.用Iterable创建DataStream(不支持)
    //val ds12: DataStream[String] = senv.fromCollection(Iterable("spark", "flink"))
    //ds12.print()

    //13.用ArraySeq创建DataStream
    val ds13: DataStream[String] = senv.fromCollection(mutable.ArraySeq("spark", "flink"))
    ds13.print()

    //14.用ArrayStack创建DataStream
    val ds14: DataStream[String] = senv.fromCollection(mutable.ArrayStack("spark", "flink"))
    ds14.print()

    //15.用Map创建DataStream(不支持)
    //val ds15: DataStream[(Int, String)] = senv.fromCollection(Map(1 -> "spark", 2 -> "flink"))
    //ds15.print()

    //16.用Range创建DataStream
    val ds16: DataStream[Int] = senv.fromCollection(Range(1, 9))
    ds16.print()

    //17.用fromElements创建DataStream
    val ds17: DataStream[Long] = senv.generateSequence(1, 9)
    ds17.print()
    
===基于文件的source（File-based-source）
    //TODO 2.基于文件的source（File-based-source）
    //0.创建运行环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    //TODO 1.读取本地文件
    val text1 = env.readTextFile("data2.csv")
    text1.print()
    //TODO 2.读取hdfs文件
    val text2 = env.readTextFile("hdfs://hadoop01:9000/input/flink/README.txt")
    
    
===基于网络套接字的source（Socket-based-source）
    val source = env.socketTextStream("IP", PORT)

===基于网络套接字的source（Socket-based-source）
    val source = env.socketTextStream("IP", PORT)
    
===自定义的source（Custom-source,以kafka为例）
          Kafka基本命令：

          复制代码
           ● 查看当前服务器中的所有topic
          bin/kafka-topics.sh --list --zookeeper  hadoop01:2181
            ● 创建topic
          bin/kafka-topics.sh --create --zookeeper hadoop01:2181 --replication-factor 1 --partitions 1 --topic test
            ● 删除topic
          sh bin/kafka-topics.sh --delete --zookeeper zk01:2181 --topic test
          需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。
            ● 通过shell命令发送消息
          sh bin/kafka-console-producer.sh --broker-list hadoop01:9092 --topic test
            ● 通过shell消费消息
          bin/kafka-console-consumer.sh --zookeeper hadoop01:2181 --from-beginning --topic test1
            ● 查看消费位置
          bin/kafka-run-cla.ss.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk01:2181 --group testGroup
            ● 查看某个Topic的详情
          bin/kafka-topics.sh --topic test --describe --zookeeper zk01:2181
            ● 对分区数进行修改
          kafka-topics.sh --zookeeper  zk01 --alter --partitions 15 --topic   utopic
          复制代码
          使用flink消费kafka的消息（不规范，其实需要自己手动维护offset）：

          复制代码
          import java.util.Properties

          import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
          import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer09
          import org.apache.flink.streaming.util.serialization.SimpleStringSchema
          import org.apache.flink.api.scala._
          /**
            * Created by angel；
            */
          object DataSource_kafka {
            def main(args: Array[String]): Unit = {
              //1指定kafka数据流的相关信息
              val zkCluster = "hadoop01,hadoop02,hadoop03:2181"
              val kafkaCluster = "hadoop01:9092,hadoop02:9092,hadoop03:9092"
              val kafkaTopicName = "test"
              //2.创建流处理环境
              val env = StreamExecutionEnvironment.getExecutionEnvironment

              //3.创建kafka数据流
              val properties = new Properties()
              properties.setProperty("bootstrap.servers", kafkaCluster)
              properties.setProperty("zookeeper.connect", zkCluster)
              properties.setProperty("group.id", kafkaTopicName)

              val kafka09 = new FlinkKafkaConsumer09[String](kafkaTopicName,
                new SimpleStringSchema(), properties)
              //4.添加数据源addSource(kafka09)
              val text = env.addSource(kafka09).setParallelism(4)

              /**
                * test#CS#request http://b2c.csair.com/B2C40/query/jaxb/direct/query.ao?t=S&c1=HLN&c2=CTU&d1=2018-07-12&at=2&ct=2&inf=1#CS#POST#CS#application/x-www-form-urlencoded#CS#t=S&json={'adultnum':'1','arrcity':'NAY','childnum':'0','depcity':'KHH','flightdate':'2018-07-12','infantnum':'2'}#CS#http://b2c.csair.com/B2C40/modules/bookingnew/main/flightSelectDirect.html?t=R&c1=LZJ&c2=MZG&d1=2018-07-12&at=1&ct=2&inf=2#CS#123.235.193.25#CS#Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1#CS#2018-01-19T10:45:13:578+08:00#CS#106.86.65.18#CS#cookie
                * */
              val values: DataStream[ProcessedData] = text.map{
                line =>
                  var encrypted = line
                  val values = encrypted.split("#CS#")
                  val valuesLength = values.length
                  var regionalRequest =  if(valuesLength > 1) values(1) else ""
                  val requestMethod = if (valuesLength > 2) values(2) else ""
                  val contentType = if (valuesLength > 3) values(3) else ""
                  //Post提交的数据体
                  val requestBody = if (valuesLength > 4) values(4) else ""
                  //http_referrer
                  val httpReferrer = if (valuesLength > 5) values(5) else ""
                  //客户端IP
                  val remoteAddr = if (valuesLength > 6) values(6) else ""
                  //客户端UA
                  val httpUserAgent = if (valuesLength > 7) values(7) else ""
                  //服务器时间的ISO8610格式
                  val timeIso8601 = if (valuesLength > 8) values(8) else ""
                  //服务器地址
                  val serverAddr = if (valuesLength > 9) values(9) else ""
                  //获取原始信息中的cookie字符串
                  val cookiesStr = if (valuesLength > 10) values(10) else ""
                  ProcessedData(regionalRequest,
                    requestMethod,
                    contentType,
                    requestBody,
                    httpReferrer,
                    remoteAddr,
                    httpUserAgent,
                    timeIso8601,
                    serverAddr,
                    cookiesStr)
              }
              values.print()
              val remoteAddr: DataStream[String] = values.map(line => line.remoteAddr)
              remoteAddr.print()

              //5.触发运算
              env.execute("flink-kafka-wordcunt")
            }
          }

          //保存结构化数据
          case class ProcessedData(regionalRequest: String,
                                   requestMethod: String,
                                   contentType: String,
                                   requestBody: String,
                                   httpReferrer: String,
                                   remoteAddr: String,
                                   httpUserAgent: String,
                                   timeIso8601: String,
                                   serverAddr: String,
                                   cookiesStr: String
                                   )
          /**
            * Flink从topic中最初的数据开始消费
            */
            consumer.setStartFromEarliest();

            /**
            * Flink从topic中指定的时间点开始消费，指定时间点之前的数据忽略
            */
            consumer.setStartFromTimestamp(1559801580000l);

            /**
            * Flink从topic中指定的offset开始，这个比较复杂，需要手动指定offset
            */
            consumer.setStartFromSpecificOffsets(offsets);

            /**
            * Flink从topic中最新的数据开始消费
            */
            consumer.setStartFromLatest();

            /**
            * Flink从topic中指定的group上次消费的位置开始消费，所以必须配置group.id参数
            */
            consumer.setStartFromGroupOffsets();
            
            
            
            ============反序列化
            1. TypeInformationSerializationSchema (andTypeInformationKeyValueSerializationSchema) ，他们会基于Flink的TypeInformation来创建schema。这对于那些从Flink写入，又从Flink读出的数据是很有用的。这种Flink-specific的反序列化会比其他通用的序列化方式带来更高的性能。

            2. JsonDeserializationSchema (andJSONKeyValueDeserializationSchema) 可以把序列化后的Json反序列化成ObjectNode，ObjectNode可以通过objectNode.get(“field”).as(Int/String/…)() 来访问指定的字段。

            3. SimpleStringSchema可以将消息反序列化为字符串。当我们接收到消息并且反序列化失败的时候，会出现以下两种情况: 1) Flink从deserialize(..)方法中抛出异常，这会导致job的失败，然后job会重启；2) 在deserialize(..) 方法出现失败的时候返回null，这会让Flink Kafka consumer默默的忽略这条消息。请注意，如果配置了checkpoint 为enable，由于consumer的失败容忍机制，失败的消息会被继续消费，因此还会继续失败，这就会导致job被不断自动重启。
            
            
            ===================
            (三) 容错机制

            当Flink的job开启了checkpoint的时候，Flink会一边消费topic的数据，一边定时的将offset和其他operator的状态记录到checkpoint中。如果遇到了job失败的情况，那么Flink将会重启job，
            从最后一个checkpoint中来恢复job的所有状态，然后从checkpoint中记录的offset开始重新对Kafka 的topic进行消费。记录offset的间隔决定了程序在失败的情况下需要回溯的最大程度。
            为了使用Flink Kafkaconsumer的容错机制，我们需要在程序中作如下的配置：
            还有一点需要注意的是，Flink只有在task slot的数量足够的情况下才可以成功的重启job，所以如果job是因为TaskManager down掉（或者无法连接到集群）导致task slot不足而失败，
            那么必须要恢复增加足够的task slot才能让job重启。而Flink on YARN 支持自动的重启丢失的YARN containers。
            
            =======================
            (四) offset提交行为的配置

            Flink KafkaConsumer允许配置向 Kafka brokers（或者向Zookeeper）提交offset的行为。需要注意的是，Flink Kafka Consumer并不依赖于这些提交回Kafka或Zookeeper的offset来保证容错。这些被提交的offset只是意味着Flink将消费的状态暴露在外以便于监控。
            Checkpointingdisabled: 此时， Flink Kafka Consumer依赖于它使用的具体的Kafka client的自动定期提交offset的行为，相应的设置是 Kafka properties中的 enable.auto.commit (或者 auto.commit.enable 对于Kafka 0.8) 以及 auto.commit.interval.ms。
            Checkpointingenabled: 在这种情况下，Flink Kafka Consumer会将offset存到checkpoint中当checkpoint 处于completed的状态时。这保证了在Kafka brokers中的committed offset和checkpointed states中的offset保持一致。通过调用setCommitOffsetOnCheckpoints(boolean)来调整 offset自动提交是否开启(默认情况下是true，即开启自动提交)。请注意，在这种情况下，配置在properties 中的offset的定时自动提交行为将会被忽略。
            
            
            ==============================
            sample
            
            object main {
                def main(args: Array[String]): Unit = {

                  //环境初始化
                  val evn = StreamExecutionEnvironment.getExecutionEnvironment
                  //flink的checkpoint的时间间隔
                  evn.enableCheckpointing(5000)
                  // 设置模式为exactly-once 默认(this is the default)
                  evn.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)

                  //kafka的consumer，test1是要消费的topic
                  val consumer = new FlinkKafkaConsumer010[String]("test", new SimpleStringSchema(), getKafkaConfig)
                  // Flink从topic中最初的数据开始消费
                  // consumer.setStartFromEarliest()
                  //设置checkpoint后在提交offset，即oncheckpoint模式
                  consumer.setCommitOffsetsOnCheckpoints(true)

                  //添加consumer
                  val stream = evn.addSource(consumer)
                  // 并发
                  // stream.setParallelism(3)
                  stream.map(new MapFunction[String, String]() {
                    override def map(value: String): String = {

                      new Date().toString + ":  " + value
                    }
                  }).print

                  //启动执行
                  evn.execute("Flink Streaming")

                }

                /**
                 * kafka相关配置
                 *
                 * @return
                 */
                def getKafkaConfig: Properties = {

                  val properties = new Properties
                  properties.setProperty("bootstrap.servers", "47.110.138.240:9092")
                  properties.setProperty("group.id", "consumer1")
                  // 如果为真，消费者的偏移量将定期在 kafka 后台提交。
                  properties.setProperty("enable.auto.commit", "false")
                  // 自动提交间隔。
                  //    properties.setProperty("auto.commit.interval.ms", "500")
                  properties.setProperty("key.deserializer", classOf[StringDeserializer].getName)
                  properties.setProperty("value.deserializer", classOf[StringDeserializer].getName)

                  properties
                }
              }

