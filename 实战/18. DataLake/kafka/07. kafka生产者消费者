------------------------------------------------------------------------
生产者
bootstrap.servers	
		用于建立与kafka集群连接的host/port组。数据将会在所有servers上均衡加载，不管哪些server是指定用于bootstrapping。这个列表仅仅影响初始化的hosts
		（用于发现全部的servers）。这个列表格式：host1:port1,host2:port2,…因为这些server仅仅是用于初始化的连接，以发现集群所有成员关系（可能会动态的变化），
		这个列表不需要包含所有的servers（你可能想要不止一个server，尽管这样，可能某个server宕机了）。如果没有server在这个列表出现，则发送数据会一直失败，
		直到列表可用。	list			高
acks				
		producer需要server接收到数据之后发出的确认接收的信号，此项配置就是指procuder需要多少个这样的确认信号。此配置实际上代表了数据备份的可用性。
		以下设置为常用选项：（1）acks=0： 设置为0表示producer不需要等待任何确认收到的信息。副本将立即加到socket buffer并认为已经发送。没有任何保障
		可以保证此种情况下server已经成功接收数据，同时重试配置不会发生作用（因为客户端不知道是否失败）回馈的offset会总是设置为-1；（2）acks=1： 
		这意味着至少要等待leader已经成功将数据写入本地log，但是并没有等待所有follower是否成功写入。这种情况下，如果follower没有成功备份数据，
		而此时leader又挂掉，则消息会丢失。（3）acks=all： 这意味着leader需要等待所有备份都成功写入日志，这种策略会保证只要有一个备份存活就不会丢失数据。
		这是最强的保证。，	string	-1	[all -1 0 1]	高
key.serializer		
		key的序列化方式，若是没有设置，同serializer.class	实现Serializer接口的class			高
value.serializer	
		value序列化类方式	实现Serializer接口的class			高
buffer.memory		
		producer可以用来缓存数据的内存大小。如果数据产生速度大于向broker发送的速度，
		producer会阻塞或者抛出异常，以“block.on.buffer.full”来表明。这项设置将和producer能够使用的总内存相关，但并不是一个硬性的限制，
		因为不是producer使用的所有内存都是用于缓存。一些额外的内存会用于压缩（如果引入压缩机制），同样还有一些用于维护请求。	
		long	33554432		高
		
compression.type	
		producer用于压缩数据的压缩类型。默认是无压缩。正确的选项值是none、gzip、snappy。压缩最好用于批量处理，批量处理消息越多，
		压缩性能越好	string	none		高
retries				
		设置大于0的值将使客户端重新发送任何数据，一旦这些数据发送失败。注意，这些重试与客户端接收到发送错误时的重试没有什么不同。
		允许重试将潜在的改变数据的顺序，如果这两个消息记录都是发送到同一个partition，则第一个消息失败第二个发送成功，则第二条消息会比第一条消息出现要早。
		int	0		高
batch.size			
		producer将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。这项配置控制默认的批量处理消息字节数。
		不会试图处理大于这个字节数的消息字节数。发送到brokers的请求将包含多个批量处理，其中会包含对每个partition的一个请求。
		较小的批量处理数值比较少用，并且可能降低吞吐量（0则会仅用批量处理）。较大的批量处理数值将会浪费更多内存空间，这样就需要分配特定批量处理数值的内存大小。	
		int	16384		高
client.id	
		当向server发出请求时，这个字符串会发送给server。目的是能够追踪请求源头，以此来允许ip/port许可列表之外的一些应用可以发送信息。
		这项应用可以设置任意字符串，因为没有任何功能性的目的，除了记录和跟踪	string	“”		中
connections.max.idle.ms	
		关闭连接空闲时间	long	540000		中
linger.ms	
		producer组将会汇总任何在请求与发送之间到达的消息记录一个单独批量的请求。通常来说，这只有在记录产生速度大于发送速度的时候才能发生。
		然而，在某些条件下，客户端将希望降低请求的数量，甚至降低到中等负载一下。这项设置将通过增加小的延迟来完成–即，不是立即发送一条记录，
		producer将会等待给定的延迟时间以允许其他消息记录发送，这些消息记录可以批量处理。这可以认为是TCP种Nagle的算法类似。
		这项设置设定了批量处理的更高的延迟边界：一旦我们获得某个partition的batch.size，他将会立即发送而不顾这项设置，
		然而如果我们获得消息字节数比这项设置要小的多，我们需要“linger”特定的时间以获取更多的消息。 这个设置默认为0，即没有延迟。设定linger.ms=5，
		例如，将会减少请求数目，但是同时会增加5ms的延迟。	long	0		中
max.block.ms	
		控制block的时长,当buffer空间不够或者metadata丢失时产生block	long	60000		中
max.request.size	
		请求的最大字节数。这也是对最大记录尺寸的有效覆盖。注意：server具有自己对消息记录尺寸的覆盖，这些尺寸和这个设置不同。
		此项设置将会限制producer每次批量发送请求的数目，以防发出巨量的请求。	int	1048576		中
partitioner.class	
		分区类	实现Partitioner 的class	class org.apache.kafka.clients.producer.internals.DefaultPartitioner		中
receive.buffer.bytes	
		socket的接收缓存空间大小,当阅读数据时使用	int	32768		中
request.timeout.ms	
		客户端将等待请求的响应的最大时间,如果在这个时间内没有收到响应，客户端将重发请求;超过重试次数将抛异常	int	3000		中
send.buffer.bytes	
		发送数据时的缓存空间大小	int	131072		中
timeout.ms	
		此配置选项控制server等待来自followers的确认的最大时间。如果确认的请求数目在此时间内没有实现，则会返回一个错误。这个超时限制是以server端度量的，没有包含请求的网络延迟	int	30000		中
max.in.flight.requests.per.connection	
		kafka可以在一个connection中发送多个请求，叫作一个flight,这样可以减少开销，但是如果产生错误，可能会造成数据的发送顺序改变,默认是5 (修改）	int	5		低
metadata.fetch.timeout.ms	
		是指我们所获取的一些元素据的第一个时间数据。元素据包含：topic，host，partitions。此项配置是指当等待元素据fetch成功完成所需要的时间，否则会跑出异常给客户端	long	60000		低
metadata.max.age.ms	
		以微秒为单位的时间，是在我们强制更新metadata的时间间隔。即使我们没有看到任何partition leadership改变。	long	300000		低
metric.reporters	
		类的列表，用于衡量指标。实现MetricReporter接口，将允许增加一些类，这些类在新的衡量指标产生时就会改变。JmxReporter总会包含用于注册JMX统计	list	none		低
metrics.num.samples	
		用于维护metrics的样本数	int	2		低
metrics.sample.window.ms	
		metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，
		我们会擦除并重写最老的窗口	long	30000		低
reconnect.backoff.ms	
		连接失败时，当我们重新连接时的等待时间。这避免了客户端反复重连	long	10		低
retry.backoff.ms	
		在试图重试失败的produce请求之前的等待时间。避免陷入发送-失败的死循环中	long	100		低
    

生产者如何保证消息被安全送到队列了？
org.apache.kafka.clients.producer.Producer的send（）方法有三个重载这个方法有一个回调函数，metadata参数不空则表明数据安全发送成功，否则发送失败。


使用方式
Kafka生产者发送消息的三种方式
Kafka是一种分布式的基于发布/订阅的消息系统，它的高吞吐量、灵活的offset是其它消息系统所没有的。

Kafka发送消息主要有三种方式：

1.发送并忘记 2.同步发送 3.异步发送+回调函数

 

下面以单节点的方式分别用三种方法发送1w条消息测试：

方式一：发送并忘记(不关心消息是否正常到达，对返回结果不做任何判断处理)

发送并忘记的方式本质上也是一种异步的方式，只是它不会获取消息发送的返回结果，这种方式的吞吐量是最高的，但是无法保证消息的可靠性：

复制代码
 1 import pickle
 2 import time
 3 from kafka import KafkaProducer
 4 
 5 producer = KafkaProducer(bootstrap_servers=['192.168.33.11:9092'],
 6                          key_serializer=lambda k: pickle.dumps(k),
 7                          value_serializer=lambda v: pickle.dumps(v))
 8 
 9 start_time = time.time()
10 for i in range(0, 10000):
11     print('------{}---------'.format(i))
12     future = producer.send('test_topic', key='num', value=i, partition=0)
13 
14 # 将缓冲区的全部消息push到broker当中
15 producer.flush()
16 producer.close()
17 
18 end_time = time.time()
19 time_counts = end_time - start_time
20 print(time_counts)
复制代码
 测试结果：1.88s

 

方式二：同步发送(通过get方法等待Kafka的响应，判断消息是否发送成功)

以同步的方式发送消息时，一条一条的发送，对每条消息返回的结果判断， 可以明确地知道每条消息的发送情况，但是由于同步的方式会阻塞，只有当消息通过get返回future对象时，才会继续下一条消息的发送：

 

复制代码
 1 import pickle
 2 import time
 3 from kafka import KafkaProducer
 4 from kafka.errors import kafka_errors
 5 
 6 producer = KafkaProducer(
 7     bootstrap_servers=['192.168.33.11:9092'],
 8     key_serializer=lambda k: pickle.dumps(k),
 9     value_serializer=lambda v: pickle.dumps(v)
10 )
11 
12 start_time = time.time()
13 for i in range(0, 10000):
14     print('------{}---------'.format(i))
15     future = producer.send(topic="test_topic", key="num", value=i)
16     # 同步阻塞,通过调用get()方法进而保证一定程序是有序的.
17     try:
18         record_metadata = future.get(timeout=10)
19         # print(record_metadata.topic)
20         # print(record_metadata.partition)
21         # print(record_metadata.offset)
22     except kafka_errors as e:
23         print(str(e))
24 
25 end_time = time.time()
26 time_counts = end_time - start_time
27 print(time_counts)
复制代码
 

测试结果：16s

 

方式三：异步发送+回调函数(消息以异步的方式发送，通过回调函数返回消息发送成功/失败)

在调用send方法发送消息的同时，指定一个回调函数，服务器在返回响应时会调用该回调函数，通过回调函数能够对异常情况进行处理，当调用了回调函数时，只有回调函数执行完毕生产者才会结束，否则一直会阻塞：

复制代码
 1 import pickle
 2 import time
 3 from kafka import KafkaProducer
 4 
 5 producer = KafkaProducer(
 6     bootstrap_servers=['192.168.33.11:9092'],
 7     key_serializer=lambda k: pickle.dumps(k),
 8     value_serializer=lambda v: pickle.dumps(v)
 9 )
10 
11 
12 def on_send_success(*args, **kwargs):
13     """
14     发送成功的回调函数
15     :param args:
16     :param kwargs:
17     :return:
18     """
19     return args
20 
21 
22 def on_send_error(*args, **kwargs):
23     """
24     发送失败的回调函数
25     :param args:
26     :param kwargs:
27     :return:
28     """
29 
30     return args
31 
32 
33 start_time = time.time()
34 for i in range(0, 10000):
35     print('------{}---------'.format(i))
36     # 如果成功,传进record_metadata,如果失败,传进Exception.
37     producer.send(
38         topic="test_topic", key="num", value=i
39     ).add_callback(on_send_success).add_errback(on_send_error)
40 
41 producer.flush()
42 producer.close()
43 
44 end_time = time.time()
45 time_counts = end_time - start_time
46 print(time_counts)
复制代码
测试结果：2.15s

 

三种方式虽然在时间上有所差别，但并不是说时间越快的越好，具体要看业务的应用场景：

场景1：如果业务要求消息必须是按顺序发送的，那么可以使用同步的方式，并且只能在一个partation上，结合参数设置retries的值让发送失败时重试，
设置max_in_flight_requests_per_connection=1，可以控制生产者在收到服务器晌应之前只能发送1个消息，从而控制消息顺序发送；

场景2：如果业务只关心消息的吞吐量，容许少量消息发送失败，也不关注消息的发送顺序，那么可以使用发送并忘记的方式，并配合参数acks=0，
这样生产者不需要等待服务器的响应，以网络能支持的最大速度发送消息；

场景3：如果业务需要知道消息发送是否成功，并且对消息的顺序不关心，那么可以用异步+回调的方式来发送消息，配合参数retries=0，
并将发送失败的消息记录到日志文件中；

-------------------------------------------------------------------------------------------------------------------------
消费者
Kafka提交offset机制
在kafka的消费者中，有一个非常关键的机制，那就是offset机制。它使得Kafka在消费的过程中即使挂了或者引发再均衡问题重新分配Partation，当下次重新恢复消费时仍然可以知道从哪里开始消费。它好比看一本书中的书签标记，每次通过书签标记(offset)就能快速找到该从哪里开始看(消费)。

Kafka对于offset的处理有两种提交方式：(1) 自动提交(默认的提交方式)   (2) 手动提交(可以灵活地控制offset)

(1) 自动提交偏移量:

Kafka中偏移量的自动提交是由参数enable_auto_commit和auto_commit_interval_ms控制的，当enable_auto_commit=True时，Kafka在消费的过程中会以频率为auto_commit_interval_ms向Kafka自带的topic(__consumer_offsets)进行偏移量提交，具体提交到哪个Partation是以算法：partation=hash(group_id)%50来计算的。

如：group_id=test_group_1，则partation=hash("test_group_1")%50=28

自动提交偏移量示例：

复制代码
 1 import pickle
 2 import uuid
 3 from kafka import KafkaConsumer
 4 
 5 consumer = KafkaConsumer(
 6     bootstrap_servers=['192.168.33.11:9092'],
 7     group_id="test_group_1",
 8     client_id="{}".format(str(uuid.uuid4())),
 9     max_poll_records=500,
10     enable_auto_commit=True,  # 默认为True 表示自动提交偏移量
11     auto_commit_interval_ms=100,  # 控制自动提交偏移量的频率 单位ms 默认是5000ms
12     key_deserializer=lambda k: pickle.loads(k),
13     value_deserializer=lambda v: pickle.loads(v)
14 )
15 
16 # 订阅消费round_topic这个主题
17 consumer.subscribe(topics=('round_topic',))
18 
19 try:
20     while True:
21         consumer_records_dict = consumer.poll(timeout_ms=1000)
22 
23         # consumer.assignment()可以获取每个分区的offset
24         for partition in consumer.assignment():
25             print('主题:{} 分区:{},需要从下面的offset开始消费:{}'.format(
26                 str(partition.topic),
27                 str(partition.partition),
28                 consumer.position(partition)
29             ))
30 
31         # 处理逻辑.
32         for k, record_list in consumer_records_dict.items():
33             print(k)
34             for record in record_list:
35                 print("topic = {},partition = {},offset = {},key = {},value = {}".format(
36                     record.topic, record.partition, record.offset, record.key, record.value)
37                 )
38 
39 finally:
40     # 调用close方法的时候会触发偏移量的自动提交 close默认autocommit=True
41     consumer.close()
复制代码
 返回结果：



在上述代码中，最后调用consumer.close()时候也会触发自动提交，因为它默认autocommit=True，源码如下：

复制代码
 1     def close(self, autocommit=True):
 2         """Close the consumer, waiting indefinitely for any needed cleanup.
 3 
 4         Keyword Arguments:
 5             autocommit (bool): If auto-commit is configured for this consumer,
 6                 this optional flag causes the consumer to attempt to commit any
 7                 pending consumed offsets prior to close. Default: True
 8         """
 9         if self._closed:
10             return
11         log.debug("Closing the KafkaConsumer.")
12         self._closed = True
13         self._coordinator.close(autocommit=autocommit)
14         self._metrics.close()
15         self._client.close()
16         try:
17             self.config['key_deserializer'].close()
18         except AttributeError:
19             pass
20         try:
21             self.config['value_deserializer'].close()
22         except AttributeError:
23             pass
24         log.debug("The KafkaConsumer has closed.")
复制代码
 

对于自动提交偏移量，如果auto_commit_interval_ms的值设置的过大，当消费者在自动提交偏移量之前异常退出，将导致kafka未提交偏移量，进而出现重复消费的问题，所以建议auto_commit_interval_ms的值越小越好。

 

(2) 手动提交偏移量:

鉴于Kafka自动提交offset的不灵活性和不精确性(只能是按指定频率的提交)，Kafka提供了手动提交offset策略。手动提交能对偏移量更加灵活精准地控制，以保证消息不被重复消费以及消息不被丢失。

对于手动提交offset主要有3种方式：1.同步提交  2.异步提交  3.异步+同步 组合的方式提交

 1.同步手动提交偏移量

同步模式下提交失败的时候一直尝试提交，直到遇到无法重试的情况下才会结束，同时同步方式下消费者线程在拉取消息会被阻塞，在broker对提交的请求做出响应之前，会一直阻塞直到偏移量提交操作成功或者在提交过程中发生异常，限制了消息的吞吐量。

复制代码
 1 """
 2 同步的方式10W条消息  4.58s
 3 """
 4 
 5 import pickle
 6 import uuid
 7 import time
 8 from kafka import KafkaConsumer
 9 
10 consumer = KafkaConsumer(
11     bootstrap_servers=['192.168.33.11:9092'],
12     group_id="test_group_1",
13     client_id="{}".format(str(uuid.uuid4())),
14     enable_auto_commit=False,  # 设置为手动提交偏移量.
15     key_deserializer=lambda k: pickle.loads(k),
16     value_deserializer=lambda v: pickle.loads(v)
17 )
18 
19 # 订阅消费round_topic这个主题
20 consumer.subscribe(topics=('round_topic',))
21 
22 try:
23     start_time = time.time()
24     while True:
25         consumer_records_dict = consumer.poll(timeout_ms=100)  # 在轮询中等待的毫秒数
26         print("获取下一轮")
27 
28         record_num = 0
29         for key, record_list in consumer_records_dict.items():
30             for record in record_list:
31                 record_num += 1
32         print("---->当前批次获取到的消息个数是:{}<----".format(record_num))
33         record_num = 0
34 
35         for k, record_list in consumer_records_dict.items():
36             for record in record_list:
37                 print("topic = {},partition = {},offset = {},key = {},value = {}".format(
38                     record.topic, record.partition, record.offset, record.key, record.value)
39                 )
40 
41         try:
42             # 轮询一个batch 手动提交一次
43             consumer.commit()  # 提交当前批次最新的偏移量. 会阻塞  执行完后才会下一轮poll
44             end_time = time.time()
45             time_counts = end_time - start_time
46             print(time_counts)
47         except Exception as e:
48             print('commit failed', str(e))
49 
50 finally:
51     consumer.close()  # 手动提交中close对偏移量提交没有影响
复制代码
 



从上述可以看出，每轮循一个批次，手动提交一次，只有当前批次的消息提交完成时才会触发poll来获取下一轮的消息，经测试10W条消息耗时4.58s

 2.异步手动提交偏移量+回调函数

 异步手动提交offset时，消费者线程不会阻塞，提交失败的时候也不会进行重试，并且可以配合回调函数在broker做出响应的时候记录错误信息。

 

复制代码
 1 """
 2 异步的方式手动提交偏移量(异步+回调函数的模式) 10W条消息 3.09s
 3 """
 4 
 5 import pickle
 6 import uuid
 7 import time
 8 from kafka import KafkaConsumer
 9 
10 consumer = KafkaConsumer(
11     bootstrap_servers=['192.168.33.11:9092'],
12     group_id="test_group_1",
13     client_id="{}".format(str(uuid.uuid4())),
14     enable_auto_commit=False,  # 设置为手动提交偏移量.
15     key_deserializer=lambda k: pickle.loads(k),
16     value_deserializer=lambda v: pickle.loads(v)
17 )
18 
19 # 订阅消费round_topic这个主题
20 consumer.subscribe(topics=('round_topic',))
21 
22 
23 def _on_send_response(*args, **kwargs):
24     """
25     提交偏移量涉及回调函数
26     :param args: args[0] --> {TopicPartition:OffsetAndMetadata}  args[1] --> Exception
27     :param kwargs:
28     :return:
29     """
30     if isinstance(args[1], Exception):
31         print('偏移量提交异常. {}'.format(args[1]))
32     else:
33         print('偏移量提交成功')
34 
35 
36 try:
37     start_time = time.time()
38     while True:
39         consumer_records_dict = consumer.poll(timeout_ms=10)
40 
41         record_num = 0
42         for key, record_list in consumer_records_dict.items():
43             for record in record_list:
44                 record_num += 1
45         print("当前批次获取到的消息个数是:{}".format(record_num))
46 
47         for record_list in consumer_records_dict.values():
48             for record in record_list:
49                 print("topic = {},partition = {},offset = {},key = {},value = {}".format(
50                     record.topic, record.partition, record.offset, record.key, record.value))
51 
52         # 避免频繁提交
53         if record_num != 0:
54             try:
55                 consumer.commit_async(callback=_on_send_response)
56             except Exception as e:
57                 print('commit failed', str(e))
58 
59         record_num = 0
60 
61 finally:
62     consumer.close()
复制代码


对于args参数：args[0]是一个dict，key是TopicPartition，value是OffsetAndMetadata，表示该主题下的partition对应的offset；args[1]在提交成功是True，提交失败时是一个Exception类。

对于异步提交，由于不会进行失败重试，当消费者异常关闭或者触发了再均衡前，如果偏移量还未提交就会造成偏移量丢失。

 3.异步+同步 组合的方式提交偏移量

针对异步提交偏移量丢失的问题，通过对消费者进行异步批次提交并且在关闭时同步提交的方式，这样即使上一次的异步提交失败，通过同步提交还能够进行补救，同步会一直重试，直到提交成功。

复制代码
 1 """
 2 同步和异步组合的方式提交偏移量
 3 """
 4 
 5 import pickle
 6 import uuid
 7 import time
 8 from kafka import KafkaConsumer
 9 
10 consumer = KafkaConsumer(
11     bootstrap_servers=['192.168.33.11:9092'],
12     group_id="test_group_1",
13     client_id="{}".format(str(uuid.uuid4())),
14     enable_auto_commit=False,  # 设置为手动提交偏移量.
15     key_deserializer=lambda k: pickle.loads(k),
16     value_deserializer=lambda v: pickle.loads(v)
17 )
18 
19 # 订阅消费round_topic这个主题
20 consumer.subscribe(topics=('round_topic',))
21 
22 
23 def _on_send_response(*args, **kwargs):
24     """
25     提交偏移量涉及的回调函数
26     :param args:
27     :param kwargs:
28     :return:
29     """
30     if isinstance(args[1], Exception):
31         print('偏移量提交异常. {}'.format(args[1]))
32     else:
33         print('偏移量提交成功')
34 
35 
36 try:
37     start_time = time.time()
38     while True:
39         consumer_records_dict = consumer.poll(timeout_ms=100)
40 
41         record_num = 0
42         for key, record_list in consumer_records_dict.items():
43             for record in record_list:
44                 record_num += 1
45         print("---->当前批次获取到的消息个数是:<----".format(record_num))
46         record_num = 0
47 
48         for k, record_list in consumer_records_dict.items():
49             print(k)
50             for record in record_list:
51                 print("topic = {},partition = {},offset = {},key = {},value = {}".format(
52                     record.topic, record.partition, record.offset, record.key, record.value)
53                 )
54 
55         try:
56             # 轮询一个batch 手动提交一次
57             consumer.commit_async(callback=_on_send_response)
58             end_time = time.time()
59             time_counts = end_time - start_time
60             print(time_counts)
61         except Exception as e:
62             print('commit failed', str(e))
63 
64 except Exception as e:
65     print(str(e))
66 finally:
67     try:
68         # 同步提交偏移量,在消费者异常退出的时候再次提交偏移量,确保偏移量的提交.
69         consumer.commit()
70         print("同步补救提交成功")
71     except Exception as e:
72         consumer.close()
复制代码
通过finally在最后不管是否异常都会触发consumer.commit()来同步补救一次，确保偏移量不会丢失



==============================================================================================
Kafka新版消费者API示例（二）

Kafka提供两种手动提交方式：
1.异步提交(commitAsync)：

   异步模式下，提交失败也不会尝试提交。消费者线程不会被阻塞，因为异步操作，可能在提交偏移量操作结果未返回时就开始下一次拉取操作。

2.同步提交(CommitSync)：

    同步模式下，提交失败时一直尝试提交，直到遇到无法重试才结束。同步方式下，消费者线程在拉取消息时会被阻塞，直到偏移量提交操作成功或者在提交过程中发生错误。

实现手动提交前需要在创建消费者时关闭自动提交，设置enable.auto.commit=false。

由于异步提交不会等消费偏移量提交成功后再拉取下一次消息，因此异步提交提供了一个偏移量提交回调方法commitAsync(OffsetCommitCallback callback)。提交偏移量完成之后会回调OffsetCommitCallback接口的onComplete()方法

示例代码：

package com.simon.kafka.consumer.newconsumer;
 
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
 
import java.util.*;
 
/**
 * Created by Simon on 2018/11/5.
 */
public class KafkaConsumerAsync {
 
    public static void main(String[] args) throws InterruptedException {
 
        // 1、准备配置文件
        String kafkas = "192.168.1.100:9092,192.168.1.100:9093,192.168.1.100:9094";
        Properties props = new Properties();
        //kafka连接信息
        props.put("bootstrap.servers",kafkas);
        //消费者组id
        props.put("group.id", "test_group");
        //是否自动提交offset
        props.put("enable.auto.commit", "false");
        //在没有offset的情况下采取的拉取策略
        props.put("auto.offset.reset", "none");
        //自动提交时间间隔
        props.put("auto.commit.interval.ms", "1000");
        //设置一次fetch请求取得的数据最大为1k
        props.put("fetch.max.bytes", "1024");
        //key反序列化
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        //value反序列化
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
 
        String topic = "test";
        // 2、创建KafkaConsumer
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        // 3、订阅数据，不给定监听器
        consumer.subscribe(Collections.singleton(topic));
 
        try{
            //最少处理100条
            int minCommitSize = 100;
            //定义计数器
            int icount = 0;
            // 4、获取数据
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(100);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("topic = %s,partition = %d,offset = %d, key = %s, value = %s%n", record.topic(), record.partition(),record.offset(), record.key(), record.value());
                    icount++;
                }
                Thread.sleep(5000);
            
                //在业务逻辑处理成功后提交offset
                if(icount >= minCommitSize){
                    //满足最少消费100条,再进行异步提交
                    consumer.commitAsync(new OffsetCommitCallback() {
                        @Override public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
                            if(exception == null){
                                System.out.println("commit success");
                            }else {
                                //提交失败，对应处理
                                System.out.println("commit failed");
                            }
                        }
                    });
                    
                    //计数器归零
                    icount = 0 ;
                }
            }
        }catch (Exception e){
            e.printStackTrace();
        }finally {
            //关闭连接
            consumer.close();
        }
    }
}
以时间戳查询消息：
    Kafka在0.10.1.1版本上增加了时间戳索引文件。Kafka消费者API提供了一个offsetsForTimes(Map<TopicPartition, Long> timestampsToSearch)方法，参数为一个map对象，key为待查询的分区，value为待查询的时间戳。会返回一个大于等于该事件戳的第一条消息对应的偏移量和时间戳。若待查询分区不存在，会一直阻塞。

 示例：

   将kafka-client的maven依赖改为1.0.0 。在0.10.0.1中无法引入OffsetAndTimestamp类

 <!--引入kafka-clients-->
    <!--<dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka-clients</artifactId>
      <version>0.10.0.1</version>
    </dependency>-->
 
    <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka-clients</artifactId>
      <version>1.0.0</version>
    </dependency>
代码：

package com.simon.kafka.consumer.newconsumer;
 
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
 
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
 
/**
 * Created by Simon on 2018/11/5.
 */
public class KafkaConsumerTimestamps {
 
    public static void main(String[] args) throws InterruptedException {
 
        // 1、准备配置文件
        String kafkas = "192.168.1.100:9092,192.168.1.100:9093,192.168.1.100:9094";
        Properties props = new Properties();
        //kafka连接信息
        props.put("bootstrap.servers",kafkas);
        //消费者组id
        props.put("group.id", "test_group");
        //客户端id
        props.put("client.id", "test_group");
        //是否自动提交offset
        props.put("enable.auto.commit", "true");
        //在没有offset的情况下采取的拉取策略
        props.put("auto.offset.reset", "none");
        //自动提交时间间隔
        props.put("auto.commit.interval.ms", "1000");
        //设置一次fetch请求取得的数据最大为1k
        props.put("fetch.max.bytes", "1024");
        //key反序列化
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        //value反序列化
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
 
        String topic = "test";
        // 2、创建KafkaConsumer
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        // 3、订阅主题
        TopicPartition topicPartition = new TopicPartition(topic,0);
        consumer.assign(Collections.singleton(topicPartition));
        try{
            Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
 
            // 设置查询12 小时之前消息的偏移量
            timestampsToSearch.put(topicPartition, (System.currentTimeMillis() - 12 * 3600 * 1000));
 
            // 会返回时间大于等于查找时间的第一个偏移量
            Map<TopicPartition, OffsetAndTimestamp> offsetMap = consumer.offsetsForTimes(timestampsToSearch);
            OffsetAndTimestamp offsetTimestamp = null;
            // 用for 轮询，当然由于本例是查询的一个分区，因此也可以用if 处理
            for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetMap.entrySet()) {
                // 若查询时间大于时间戳索引文件中最大记录索引时间，
                // 此时value 为空,即待查询时间点之后没有新消息生成
                offsetTimestamp = entry.getValue();
                if (null != offsetTimestamp) {
                    // 重置消费起始偏移量
                    consumer.seek(topicPartition, entry.getValue().offset());
                }
            }
            while (true) {
                //4.轮询拉取消息
                ConsumerRecords<String, String> records = consumer.poll(100);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("topic = %s,partition = %d,offset = %d, key = %s, value = %s%n", record.topic(), record.partition(),record.offset(), record.key(), record.value());
 
                }
            }
        }catch (Exception e){
            e.printStackTrace();
        }finally {
            //关闭连接
            consumer.close();
        }
    }
}
由于集群环境已选型为kafka0.10.0.1，本次无法按指定时间戳拉取，报错信息为不支持当前broker版本。

速度控制：
   应用场景中我们可能需要暂停某些分区消费，先消费其他分区，当达到某个条件再恢复该分区消费。

Kafka提供两种方法用于速度控制的方法：

     1.pause(Collection<TopicPartition> partitions)：暂停某些分区在拉取操作时返回数据给客户端

//无返回值
consumer.pause(Collections.singleton(topicPartition));
     2.resume(Collection<TopicPartition> partitions)：恢复某些分区向客户端返回数据

//无返回值
consumer.resume(Collections.singleton(topicPartition));


==========================================================================================================
Kafka 新版消费者 API（三）：以时间戳查询消息和消费速度控制
1. 以时间戳查询消息
(1) Kafka 新版消费者基于时间戳索引消费消息
kafka 在 0.10.1.1 版本增加了时间索引文件，因此我们可以根据时间戳来访问消息。
 如以下需求：从半个小时之前的offset处开始消费消息，代码示例如下:

package com.bonc.rdpe.kafka110.consumer;

import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndTimestamp;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.TopicPartition;

public class TimestampConsumer {
        
    public static void main(String[] args) {
        
        Properties props = new Properties();
        props.put("bootstrap.servers", "rdpecore4:9092,rdpecore5:9092,rdpecore6:9092");
        props.put("group.id", "dev3-yangyunhe-topic001-group001");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        String topic = "dev3-yangyunhe-topic001";
        
        try {
            // 获取topic的partition信息
            List<PartitionInfo> partitionInfos = consumer.partitionsFor(topic);
            List<TopicPartition> topicPartitions = new ArrayList<>();
            
            Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
            DateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
            Date now = new Date();
            long nowTime = now.getTime();
            System.out.println("当前时间: " + df.format(now));
            long fetchDataTime = nowTime - 1000 * 60 * 30;  // 计算30分钟之前的时间戳
            
            for(PartitionInfo partitionInfo : partitionInfos) {
                topicPartitions.add(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()));
                timestampsToSearch.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), fetchDataTime);
            }
            
            consumer.assign(topicPartitions);
            
            // 获取每个partition一个小时之前的偏移量
            Map<TopicPartition, OffsetAndTimestamp> map = consumer.offsetsForTimes(timestampsToSearch);
            
            OffsetAndTimestamp offsetTimestamp = null;
            System.out.println("开始设置各分区初始偏移量...");
            for(Map.Entry<TopicPartition, OffsetAndTimestamp> entry : map.entrySet()) {
                // 如果设置的查询偏移量的时间点大于最大的索引记录时间，那么value就为空
                offsetTimestamp = entry.getValue();
                if(offsetTimestamp != null) {
                    int partition = entry.getKey().partition();
                    long timestamp = offsetTimestamp.timestamp();
                    long offset = offsetTimestamp.offset();
                    System.out.println("partition = " + partition + 
                            ", time = " + df.format(new Date(timestamp))+ 
                            ", offset = " + offset);
                    // 设置读取消息的偏移量
                    consumer.seek(entry.getKey(), offset);
                }
            }
            System.out.println("设置各分区初始偏移量结束...");
            
            while(true) {
                ConsumerRecords<String, String> records = consumer.poll(1000);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("partition = " + record.partition() + ", offset = " + record.offset());
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            consumer.close();
        }
    }
}

运行结果：
当前时间: 2018-07-16 10:15:09
开始设置各分区初始偏移量...
partition = 2, time = 2018-07-16 09:45:10, offset = 727
partition = 0, time = 2018-07-16 09:45:09, offset = 727
partition = 1, time = 2018-07-16 09:45:10, offset = 727
设置各分区初始偏移量结束...
partition = 1, offset = 727
partition = 1, offset = 728
partition = 1, offset = 729
......
partition = 2, offset = 727
partition = 2, offset = 728
partition = 2, offset = 729
......
partition = 0, offset = 727
partition = 0, offset = 728
partition = 0, offset = 729
......
说明：基于时间戳查询消息，consumer 订阅 topic 的方式必须是 Assign 
(2) Spark基于kafka时间戳索引读取数据并加载到RDD中
以下为一个通用的，spark读取kafka中某段时间之前到执行程序此刻的时间范围内的数据并加载到RDD中的方法：

package com.bonc.utils

import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.common.TopicPartition
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.{KafkaUtils, OffsetRange}

import scala.collection.JavaConversions._

/**
  * Author: YangYunhe
  * Description: 
  * Create: 2018-06-29 11:35
  */
object SparkKafkaUtils {

  /**
    * 从 Kafka 中取数据加载到 RDD 中
    * @param sc SparkContext
    * @param topic Kafka 的 Topic
    * @param numDays 取距离此刻多少天之前的数据，例如，这个参数为 3，那么取此刻和3天之前相同时刻范围内的数据
    * @param kafkaParams Kafka的配置参数，用于创建生产者和作为参数传给 KafkaUtils.createRDD
    * @return
    */
  def createKafkaRDDByTimeRange(sc: SparkContext, topic: String, numDays: Int, kafkaParams: java.util.HashMap[String, Object]): RDD[String] = {

    val startFetchTime = DateUtils.daysAgo(numDays)
    val startFetchTimeStr = DateUtils.parseLong2String(startFetchTime, DateUtils.DATE_TIME_FORMAT_STR)
    println(s"starting fetch data in kafka with time range [${startFetchTimeStr}——${DateUtils.nowStr()}]")

    val consumer = new KafkaConsumer[String, String](kafkaParams)

    val partitionInfos = consumer.partitionsFor(topic)
    val topicPartitions = scala.collection.mutable.ArrayBuffer[TopicPartition]()
    val timestampsToSearch = scala.collection.mutable.Map[TopicPartition, java.lang.Long]()
    val offsetRanges = scala.collection.mutable.ArrayBuffer[OffsetRange]()

    for(partitionInfo <- partitionInfos) {
      topicPartitions += new TopicPartition(partitionInfo.topic, partitionInfo.partition)
    }

    val topicPartitionLongMap = consumer.endOffsets(topicPartitions)

    for(topicPartition <- topicPartitions) {
      timestampsToSearch(topicPartition) = startFetchTime
    }

    val topicPartitionOffsetAndTimestampMap = consumer.offsetsForTimes(timestampsToSearch)

    for((k, v) <- topicPartitionOffsetAndTimestampMap) {
      offsetRanges += OffsetRange.create(topic, k.partition(), v.offset(), topicPartitionLongMap.get(k))
    }

    KafkaUtils.createRDD[String, String](sc, kafkaParams, offsetRanges.toArray, PreferConsistent).map(_.value)

  }
}

使用方法：

def main(args: Array[String]): Unit = {
    val kafkaParams = new JHashMap[String, Object]()
    kafkaParams.put("bootstrap.servers", bootstrapServers)
    kafkaParams.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    kafkaParams.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    
    // 这里就取到了kafka中3天的数据到RDD中
    val rdd = SparkKafkaUtils.createKafkaRDDByTimeRange(sc, "topic", 3, kafkaParams)
    
    rdd.map(x => {
        // 其他操作
        ......
    })

}
2. 消费速度控制
在有些场景可以需要暂停某些分区消费，达到一定条件再恢复对这些分区的消费，可以使用pause()方法暂停消费，resume()方法恢复消费，示例代码如下：

package com.bonc.rdpe.kafka110.consumer;

import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Arrays;
import java.util.Collections;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;

/**
 * @author YangYunhe
 * @date 2018-07-16 15:13:11
 * @description: 消费速度控制
 */
public class PauseAndResumeConsumer {
    
    private static final DateFormat df = new SimpleDateFormat("HH");
    
    public static String getTimeRange() {
        long now = System.currentTimeMillis();
        String hourStr = df.format(now);
        int hour;
        if(hourStr.charAt(0) == '0') {
            hour = Integer.parseInt(hourStr.substring(1, 1));
        }else {
            hour = Integer.parseInt(hourStr);
        }
        if(hour >= 0 && hour < 8) {
            return "00:00-08:00";
        }else if(hour >= 8 && hour < 16) {
            return "08:00-16:00";
        }else {
            return "16:00-00:00";
        }
    }
    
    public static void main(String[] args) throws Exception {

        Properties props = new Properties();
        props.put("bootstrap.servers", "rdpecore4:9092,rdpecore5:9092,rdpecore6:9092");
        props.put("group.id", "dev3-yangyunhe-topic001-group003");
        props.put("auto.offset.reset", "earliest");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        TopicPartition partition0 = new TopicPartition("dev3-yangyunhe-topic001", 0);
        TopicPartition partition1 = new TopicPartition("dev3-yangyunhe-topic001", 1);
        TopicPartition partition2 = new TopicPartition("dev3-yangyunhe-topic001", 2);
        
        consumer.assign(Arrays.asList(new TopicPartition[]{partition0, partition1, partition2}));
        
        try {
            while (true) {
                // 00:00-08:00从partition0读取数据
                if(getTimeRange() == "00:00-08:00") {
                    consumer.pause(Arrays.asList(new TopicPartition[]{partition1, partition2}));
                    consumer.resume(Collections.singletonList(partition0));
                // 08:00-16:00从partition1读取数据
                }else if(getTimeRange() == "08:00-16:00") {
                    consumer.pause(Arrays.asList(new TopicPartition[]{partition0, partition2}));
                    consumer.resume(Collections.singletonList(partition1));
                // 16:00-00:00从partition2读取数据
                }else {
                    consumer.pause(Arrays.asList(new TopicPartition[]{partition0, partition1}));
                    consumer.resume(Collections.singletonList(partition2));
                }
                
                ConsumerRecords<String, String> records = consumer.poll(1000);
            
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("topic = " + record.topic() + ", partition = " + record.partition());
                    System.out.println("offset = " + record.offset());
                }
            }
        } finally {
            consumer.close();
        }
    }

}

结果：(我运行程序的时间是18:27,所以只会消费partition2中的消息)
topic = dev3-yangyunhe-topic001, partition = 2
offset = 0
topic = dev3-yangyunhe-topic001, partition = 2
offset = 1
topic = dev3-yangyunhe-topic001, partition = 2
offset = 2
......


=================================================================================================
Apache Kafka消费者组subscribe和assign的正确使用
使用Apache Kafka 消费者组时，有一个为消费者分配对应分区partition的过程，我们可以使用“自动”subscribe和“手动”assign的方式。

同时进行“自动”和“手动”的分区分配是会互相影响的，有时会把事情搞糟。正确的使用，首先要了解这两种方式的场景。
消费者组的使用场景
Kafka里的消费者组有两个使用的场景：

“队列模式”：在同一组的消费者共同消费一个主题的所有消息，而且确保一条消息只被一个消费者处理。一个主题的所有的分区会和一个消费组的所有消费者做关联：每个消费者和一到多个分区做关联，接收它们的消息。反向说，一个分区只会与一个消费者关联，它的消息不会被其它的消费者接收。
最开始只有一个消费者时，所有的分区都分配给了它。当消息的规模增加时，我们就需要扩展消费者的数量，水平扩展处理能力，一直可以达到每个消费者只关联一个分区。大于分区数的消费者是会处在空闲状态，因为没有分配任何的分区。
“发布/订阅模式” 创建不同的消费者组意味一个主题的消息会发送给所有订阅它的消费者组，然后消费者组依照前面共同协作的场景进行分配。这往往是因为我们有不同的应用需求，比如一批交易数据，资金系统、ERP系统会消费它而风险监控也需要同时消费它。这就实现了数据的透明异步共用。
在两个场景中，消费者组有个重要的功能：rebalancing。当一个新的消费者加入一个组，如果还有有效的分区（消费者数<=主题分区数），会开始一个重新均衡分配的操作，会将一个已关联的分区（它的原消费者仍保有至少一个分区）重新分配给新加入的消费者。同样的，当一个消费者因为各种原因离开这个组，它的所有分区会被分配给剩下的消费者。

“自动” OR “手动”
前面所说的自动分配是指在 KafkaConsumer API中的subscribe()方法。这个方法强制要求你为消费者设置一个消费者组，group.id参数不能为空。而你不需要处理分区的分配问题。
而对应subscribe()方法，你可以采用手动的方式，指定消费者读取哪个主题分区，则：assign() 方法。当你需要精确地控制消息处理的负载，也能确定哪个分区有哪些消息时，这种手动的方式会很有用。但这时Kafka也无法提供rebalancing的功能了。而且在使用手动的方式时，你可以不指定消费者组，group.id为空。
两种方式都各有适用场景，但同时不建议同时使用两种方式，这会带来风险。假设一个消费者组G1，组内只有一个消费者C1，订阅subscribe了一个具有两个分区P1、P2的主题T1。这时在G1新增一个消费者C2，用assign的方式关联P1和P2。视乎一切都可行，但其实是糟糕的情况。本质上，使用场景被混淆了，你无法确定G1是在共同协助还是在进行发布/订阅。实际使用中，offset的提交格式是这样的：
    key = [group, topic, partition]
    value = offset

注意Key中并未区分消费者，C1和C2会对同一个key会脏写。代表着C1或C2奔溃重启时可能会拿到对方重写覆盖的offset，消息也会有丢失。
总结
最好是使用subscribe()方法，让分区自动分配。毕竟Kafka的消费者组机制已经很优秀，为我们节省了很多功夫。哪怕你需要采用assign()指定的方式，也应该设置好对应的消费者组。尽量别混合使用。

